{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e574e226",
   "metadata": {},
   "source": [
    "# Execution Path Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b067c063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "790a2741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from pathlib import Path, PurePosixPath\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d3bd0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17. 24. 36. 44. 16.]\n",
      " [24. 36. 44. 16. 31.]\n",
      " [36. 44. 16. 31. 30.]\n",
      " ...\n",
      " [20. 20. 20. 20. 20.]\n",
      " [20. 20. 20. 20. 20.]\n",
      " [20. 20. 20. 20. 20.]]\n",
      "=================\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch 1/50\n",
      "6891/6891 - 5s - loss: 0.1583 - accuracy: 0.9819 - val_loss: 0.0130 - val_accuracy: 0.9995\n",
      "Epoch 2/50\n",
      "6891/6891 - 5s - loss: 0.0567 - accuracy: 0.9907 - val_loss: 0.0023 - val_accuracy: 0.9999\n",
      "Epoch 3/50\n",
      "6891/6891 - 5s - loss: 0.0497 - accuracy: 0.9909 - val_loss: 0.0204 - val_accuracy: 0.9994\n",
      "Epoch 4/50\n",
      "6891/6891 - 5s - loss: 0.0472 - accuracy: 0.9911 - val_loss: 0.0019 - val_accuracy: 0.9999\n",
      "Epoch 5/50\n",
      "6891/6891 - 5s - loss: 0.0447 - accuracy: 0.9914 - val_loss: 0.0026 - val_accuracy: 0.9999\n",
      "Epoch 6/50\n",
      "6891/6891 - 5s - loss: 0.0435 - accuracy: 0.9917 - val_loss: 0.0028 - val_accuracy: 0.9999\n",
      "Epoch 7/50\n",
      "6891/6891 - 5s - loss: 0.0426 - accuracy: 0.9914 - val_loss: 0.0044 - val_accuracy: 0.9999\n",
      "Epoch 8/50\n",
      "6891/6891 - 5s - loss: 0.0409 - accuracy: 0.9918 - val_loss: 0.0029 - val_accuracy: 0.9999\n",
      "Epoch 9/50\n",
      "6891/6891 - 5s - loss: 0.0398 - accuracy: 0.9917 - val_loss: 0.0026 - val_accuracy: 0.9999\n",
      "Epoch 10/50\n",
      "6891/6891 - 5s - loss: 0.0392 - accuracy: 0.9919 - val_loss: 0.0022 - val_accuracy: 0.9999\n",
      "Epoch 11/50\n",
      "6891/6891 - 5s - loss: 0.0376 - accuracy: 0.9920 - val_loss: 0.0019 - val_accuracy: 0.9999\n",
      "Epoch 12/50\n",
      "6891/6891 - 5s - loss: 0.0363 - accuracy: 0.9920 - val_loss: 0.0061 - val_accuracy: 0.9999\n",
      "Epoch 13/50\n",
      "6891/6891 - 5s - loss: 0.0358 - accuracy: 0.9920 - val_loss: 0.0021 - val_accuracy: 0.9999\n",
      "Epoch 14/50\n",
      "6891/6891 - 5s - loss: 0.0344 - accuracy: 0.9922 - val_loss: 0.0045 - val_accuracy: 0.9999\n",
      "Epoch 15/50\n",
      "6891/6891 - 5s - loss: 0.0347 - accuracy: 0.9922 - val_loss: 0.0017 - val_accuracy: 0.9999\n",
      "Epoch 16/50\n",
      "6891/6891 - 5s - loss: 0.0348 - accuracy: 0.9925 - val_loss: 0.0018 - val_accuracy: 0.9999\n",
      "Epoch 17/50\n",
      "6891/6891 - 5s - loss: 0.0332 - accuracy: 0.9926 - val_loss: 0.0025 - val_accuracy: 0.9999\n",
      "Epoch 18/50\n",
      "6891/6891 - 5s - loss: 0.0328 - accuracy: 0.9925 - val_loss: 0.0098 - val_accuracy: 0.9999\n",
      "Epoch 19/50\n",
      "6891/6891 - 5s - loss: 0.0325 - accuracy: 0.9926 - val_loss: 0.0014 - val_accuracy: 0.9999\n",
      "Epoch 20/50\n",
      "6891/6891 - 5s - loss: 0.0334 - accuracy: 0.9929 - val_loss: 0.0018 - val_accuracy: 0.9999\n",
      "Epoch 21/50\n",
      "6891/6891 - 5s - loss: 0.0328 - accuracy: 0.9926 - val_loss: 0.0044 - val_accuracy: 0.9999\n",
      "Epoch 22/50\n",
      "6891/6891 - 5s - loss: 0.0324 - accuracy: 0.9927 - val_loss: 0.0027 - val_accuracy: 0.9999\n",
      "Epoch 23/50\n",
      "6891/6891 - 5s - loss: 0.0324 - accuracy: 0.9927 - val_loss: 0.0015 - val_accuracy: 0.9999\n",
      "Epoch 24/50\n",
      "6891/6891 - 5s - loss: 0.0325 - accuracy: 0.9925 - val_loss: 0.0015 - val_accuracy: 0.9999\n",
      "Epoch 25/50\n",
      "6891/6891 - 5s - loss: 0.0316 - accuracy: 0.9927 - val_loss: 0.0025 - val_accuracy: 0.9999\n",
      "Epoch 26/50\n",
      "6891/6891 - 5s - loss: 0.0316 - accuracy: 0.9929 - val_loss: 0.0146 - val_accuracy: 0.9999\n",
      "Epoch 27/50\n",
      "6891/6891 - 5s - loss: 0.0330 - accuracy: 0.9926 - val_loss: 0.0016 - val_accuracy: 0.9999\n",
      "Epoch 28/50\n",
      "6891/6891 - 5s - loss: 0.0312 - accuracy: 0.9929 - val_loss: 0.0013 - val_accuracy: 0.9999\n",
      "Epoch 29/50\n",
      "6891/6891 - 5s - loss: 0.0321 - accuracy: 0.9927 - val_loss: 0.0014 - val_accuracy: 0.9999\n",
      "Epoch 30/50\n",
      "6891/6891 - 5s - loss: 0.0313 - accuracy: 0.9929 - val_loss: 0.0022 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "    # read the data\n",
    "    #current_path = Path(__file__).resolve().parent\n",
    "    df = pd.read_pickle(PurePosixPath('RtpDump_1.pkl'))\n",
    "    encoder_file = PurePosixPath(\"encoder.save\")\n",
    "    model_file = PurePosixPath(\"model.h5\")\n",
    "\n",
    "    X, one_hot_y, class_num = encode_key(df['log key'], 'encoder.save', 5)\n",
    "    model = model_build(5, class_num)\n",
    "    classifier = fit_eval(model, model_file ,30, X, one_hot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55f88744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_build(5, class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6239715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = fit_eval(model, model_file ,30, X, one_hot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3047e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "================================\n",
      "WARNING:tensorflow:From <ipython-input-11-0a0c965e9c4c>:148: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "7/7 [==============================] - 0s 712us/step\n",
      "WARNING:tensorflow:From <ipython-input-11-0a0c965e9c4c>:153: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "7/7 [==============================] - 0s 713us/step\n",
      "[(14, 0.29119253), (32, 0.16001934), (47, 0.14783965), (31, 0.13416559)]\n",
      "[(30, 0.3669674), (25, 0.15219292)]\n",
      "[(30, 0.22291204),\n",
      " (42, 0.14844754),\n",
      " (41, 0.13239536),\n",
      " (51, 0.11444213),\n",
      " (25, 0.11051576)]\n",
      "[(25, 0.11206782)]\n",
      "[(30, 0.20514344),\n",
      " (42, 0.17685567),\n",
      " (41, 0.14477387),\n",
      " (51, 0.10431748),\n",
      " (25, 0.10390316)]\n",
      "[(30, 0.20582317),\n",
      " (42, 0.14969486),\n",
      " (25, 0.11945817),\n",
      " (41, 0.11932996),\n",
      " (51, 0.115571454)]\n",
      "[(30, 0.28738368), (51, 0.15108079), (41, 0.12739275), (47, 0.10691818)]\n",
      "[(30, 0.44234863), (47, 0.14320025), (25, 0.116135426)]\n",
      "[(30, 0.23724343), (41, 0.17307942), (42, 0.17184849), (51, 0.10272528)]\n",
      "[(30, 0.21678382), (42, 0.18576184), (41, 0.16883543)]\n",
      "[(30, 0.19327863), (51, 0.15418364), (3, 0.14399758), (47, 0.13450266)]\n",
      "[(30, 0.43781558), (47, 0.15189359), (25, 0.11085156)]\n",
      "[(25, 0.11471049), (30, 0.101542704)]\n",
      "[(30, 0.18311715),\n",
      " (15, 0.15987565),\n",
      " (9, 0.12412525),\n",
      " (32, 0.101513594),\n",
      " (47, 0.101471104)]\n",
      "[(30, 0.37898487),\n",
      " (51, 0.13764559),\n",
      " (47, 0.12609814),\n",
      " (21, 0.105470434),\n",
      " (14, 0.10413644)]\n",
      "[(32, 0.18033741), (30, 0.14469045), (43, 0.12096905)]\n",
      "[(32, 0.56215656), (30, 0.34654078)]\n",
      "[(30, 0.44229114), (21, 0.19795543), (47, 0.124687456), (14, 0.121608965)]\n",
      "[(30, 0.37498853), (15, 0.22353226), (47, 0.10049842)]\n",
      "[(30, 0.15889238), (25, 0.13587798), (51, 0.11404918), (42, 0.10787358)]\n",
      "[(30, 0.21661896), (41, 0.2160448), (42, 0.20313558)]\n",
      "[(30, 0.38167787), (41, 0.32518864)]\n",
      "[(30, 0.20042059),\n",
      " (47, 0.18175867),\n",
      " (3, 0.17900904),\n",
      " (15, 0.13822751),\n",
      " (51, 0.11972705)]\n",
      "[(30, 0.34307277), (41, 0.13997956), (47, 0.13609685), (51, 0.1347799)]\n",
      "[(15, 0.35703084), (47, 0.15864134), (30, 0.15662123)]\n",
      "[(30, 0.4448981), (47, 0.39577302)]\n",
      "[(30, 0.22294433),\n",
      " (42, 0.1535192),\n",
      " (51, 0.13694146),\n",
      " (41, 0.13236846),\n",
      " (32, 0.10894679)]\n",
      "[(30, 0.39409766), (15, 0.16560316), (47, 0.10904878)]\n",
      "[(30, 0.22956067),\n",
      " (51, 0.14797273),\n",
      " (15, 0.14362745),\n",
      " (3, 0.12318367),\n",
      " (47, 0.11782134)]\n",
      "[(30, 0.25279167), (41, 0.24094512), (42, 0.18684718)]\n",
      "[(30, 0.34548423), (41, 0.2744246), (51, 0.10961403)]\n",
      "[(30, 0.39470795), (47, 0.15069836), (41, 0.13288131), (51, 0.10751537)]\n",
      "[(30, 0.36154553), (47, 0.14752239), (41, 0.12905079), (51, 0.12034342)]\n",
      "[(15, 0.3759036), (30, 0.18255478), (47, 0.16776119)]\n",
      "[(30, 0.4711181), (47, 0.23660314)]\n",
      "[(30, 0.2068808), (32, 0.17072143), (51, 0.1429793)]\n",
      "[(30, 0.42923027),\n",
      " (32, 0.13289174),\n",
      " (41, 0.10375374),\n",
      " (47, 0.10245621),\n",
      " (15, 0.101032205)]\n",
      "[(33, 0.84322184)]\n",
      "[(30, 0.4209748),\n",
      " (51, 0.12843256),\n",
      " (47, 0.118572),\n",
      " (21, 0.107057236),\n",
      " (14, 0.101357244)]\n",
      "[(30, 0.23302163), (29, 0.18598172), (32, 0.12625442), (41, 0.12208033)]\n",
      "[(51, 0.15284622),\n",
      " (32, 0.14926311),\n",
      " (3, 0.13844053),\n",
      " (47, 0.12866376),\n",
      " (14, 0.12407386),\n",
      " (30, 0.10769613)]\n",
      "[(30, 0.45834944), (25, 0.20457116), (32, 0.11844301)]\n",
      "[(30, 0.24660408), (41, 0.20394441), (42, 0.1804287)]\n",
      "[(30, 0.23945771), (41, 0.1803539), (42, 0.1745526), (51, 0.10050237)]\n",
      "[(30, 0.3552414), (41, 0.14663942), (47, 0.13525376), (51, 0.13431132)]\n",
      "[(30, 0.25116098), (41, 0.12975572), (51, 0.11727127), (42, 0.10976482)]\n",
      "[(30, 0.4520954), (47, 0.17883506)]\n",
      "[(30, 0.26122448), (41, 0.19588408), (42, 0.1610088)]\n",
      "[(30, 0.3385225), (41, 0.15884034), (51, 0.13505568), (47, 0.12914638)]\n",
      "[(30, 0.50690997), (47, 0.23256196)]\n",
      "[(30, 0.4862381), (47, 0.27175522)]\n",
      "[(30, 0.5041853), (47, 0.23005576)]\n",
      "[(30, 0.50223905), (47, 0.22829819)]\n",
      "[(30, 0.50223905), (47, 0.22829819)]\n",
      "[(30, 0.45588785), (47, 0.21364935)]\n",
      "[(30, 0.4618301), (47, 0.19633637)]\n",
      "[(30, 0.434026), (47, 0.19940563)]\n",
      "[(30, 0.23717438), (41, 0.18003279), (42, 0.17650989), (51, 0.10014276)]\n",
      "[(30, 0.21724623), (42, 0.19171016), (41, 0.18088458)]\n",
      "[(30, 0.35658392), (41, 0.17163013), (51, 0.142539), (47, 0.12098927)]\n",
      "[(30, 0.32669798), (51, 0.14156409), (41, 0.12726456), (47, 0.1259277)]\n",
      "[(30, 0.46925843), (47, 0.2016947)]\n",
      "[(30, 0.4317385), (47, 0.20804647)]\n",
      "[(30, 0.23845181), (41, 0.18155618), (42, 0.17625833)]\n",
      "[(30, 0.21724623), (42, 0.19171016), (41, 0.18088458)]\n",
      "[(30, 0.35658392), (41, 0.17163013), (51, 0.142539), (47, 0.12098927)]\n",
      "[(30, 0.2528151), (41, 0.13624026), (42, 0.11914815), (51, 0.11320811)]\n",
      "[(30, 0.5077555), (47, 0.233351)]\n",
      "[(30, 0.1689662), (15, 0.14230403), (25, 0.10287605)]\n",
      "[(30, 0.29615697), (15, 0.1997135), (47, 0.1752859), (32, 0.113637805)]\n",
      "[(30, 0.4066929), (47, 0.18897344), (32, 0.10076479)]\n",
      "[(32, 0.15853454), (30, 0.1566316), (51, 0.11370165)]\n",
      "[(30, 0.23037125), (41, 0.17770602), (42, 0.12263955), (51, 0.114859335)]\n",
      "[(30, 0.3417241), (41, 0.19626634), (51, 0.1494409), (47, 0.10059585)]\n",
      "[(30, 0.35514107), (41, 0.1625249), (51, 0.14316943), (47, 0.12251233)]\n",
      "[(30, 0.25330117), (41, 0.1398846), (42, 0.12290618), (51, 0.11266916)]\n",
      "[(30, 0.45288154), (47, 0.22261071)]\n",
      "[(30, 0.4484558), (47, 0.17733854)]\n",
      "[(30, 0.25917202), (41, 0.1981526), (42, 0.16455054)]\n",
      "[(15, 0.33791876), (30, 0.23675364)]\n",
      "[(30, 0.46338594), (47, 0.19744338)]\n",
      "[(20, 0.30775982)]\n",
      "[(30, 0.5268132), (47, 0.3674986)]\n",
      "[(30, 0.44934037), (21, 0.22230707), (14, 0.13348523), (47, 0.12498156)]\n",
      "[(30, 0.42810246), (15, 0.19530772), (41, 0.1011565)]\n",
      "[(30, 0.27966234), (15, 0.1877514), (51, 0.12221994), (47, 0.11651748)]\n",
      "[(30, 0.26025566), (41, 0.19959006), (42, 0.16411912)]\n",
      "[(15, 0.26548615),\n",
      " (30, 0.23370157),\n",
      " (3, 0.109406985),\n",
      " (51, 0.10188967),\n",
      " (47, 0.10026334)]\n",
      "[(30, 0.48216233), (47, 0.21148895)]\n",
      "[(15, 0.351099), (30, 0.1881812), (47, 0.163213)]\n",
      "[(30, 0.32715893), (47, 0.1846804), (15, 0.16742264), (32, 0.11475874)]\n",
      "[(30, 0.45848456), (25, 0.20409414), (32, 0.11833779)]\n",
      "[(30, 0.4554804), (25, 0.21352966), (32, 0.120335504)]\n",
      "[(30, 0.43947205), (25, 0.24737261), (32, 0.12607487)]\n",
      "[(30, 0.43947205), (25, 0.24737261), (32, 0.12607487)]\n",
      "[(30, 0.22559664), (42, 0.18462017), (41, 0.17735794)]\n",
      "[(30, 0.24285784), (51, 0.13466315), (41, 0.13009004)]\n",
      "[(30, 0.25200197), (41, 0.18988775), (42, 0.16765401)]\n",
      "[(30, 0.22445357),\n",
      " (51, 0.11941473),\n",
      " (42, 0.118614204),\n",
      " (41, 0.11359631),\n",
      " (25, 0.11057821)]\n",
      "[(30, 0.44248968), (47, 0.210024)]\n",
      "[14, 32, 47, 31]\n",
      "[30, 25]\n",
      "[30, 42, 41, 51, 25]\n",
      "[25]\n",
      "That is a potential anomaly prediction: 43 from sequence [44. 16. 31. 30. 30.]\n",
      "[30, 42, 41, 51, 25]\n",
      "[30, 42, 25, 41, 51]\n",
      "[30, 51, 41, 47]\n",
      "[30, 47, 25]\n",
      "That is a potential anomaly prediction: 40 from sequence [30. 43. 30. 30. 30.]\n",
      "[30, 41, 42, 51]\n",
      "That is a potential anomaly prediction: 40 from sequence [43. 30. 30. 30. 40.]\n",
      "[30, 42, 41]\n",
      "That is a potential anomaly prediction: 21 from sequence [30. 30. 30. 40. 40.]\n",
      "[30, 51, 3, 47]\n",
      "[30, 47, 25]\n",
      "That is a potential anomaly prediction: 3 from sequence [30. 40. 40. 21. 30.]\n",
      "[25, 30]\n",
      "That is a potential anomaly prediction: 21 from sequence [40. 40. 21. 30.  3.]\n",
      "[30, 15, 9, 32, 47]\n",
      "[30, 51, 47, 21, 14]\n",
      "That is a potential anomaly prediction: 3 from sequence [21. 30.  3. 21. 30.]\n",
      "[32, 30, 43]\n",
      "That is a potential anomaly prediction: 41 from sequence [30.  3. 21. 30.  3.]\n",
      "[32, 30]\n",
      "[30, 21, 47, 14]\n",
      "[30, 15, 47]\n",
      "That is a potential anomaly prediction: 37 from sequence [30.  3. 41. 30. 21.]\n",
      "[30, 25, 51, 42]\n",
      "That is a potential anomaly prediction: 46 from sequence [ 3. 41. 30. 21. 37.]\n",
      "[30, 41, 42]\n",
      "That is a potential anomaly prediction: 50 from sequence [41. 30. 21. 37. 46.]\n",
      "[30, 41]\n",
      "[30, 47, 3, 15, 51]\n",
      "[30, 41, 47, 51]\n",
      "That is a potential anomaly prediction: 3 from sequence [37. 46. 50. 41. 30.]\n",
      "[15, 47, 30]\n",
      "That is a potential anomaly prediction: 42 from sequence [46. 50. 41. 30.  3.]\n",
      "[30, 47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That is a potential anomaly prediction: 35 from sequence [50. 41. 30.  3. 42.]\n",
      "[30, 42, 51, 41, 32]\n",
      "[30, 15, 47]\n",
      "That is a potential anomaly prediction: 41 from sequence [30.  3. 42. 35. 30.]\n",
      "[30, 51, 15, 3, 47]\n",
      "[30, 41, 42]\n",
      "That is a potential anomaly prediction: 51 from sequence [42. 35. 30. 41. 51.]\n",
      "[30, 41, 51]\n",
      "[30, 47, 41, 51]\n",
      "[30, 47, 41, 51]\n",
      "That is a potential anomaly prediction: 3 from sequence [41. 51. 51. 51. 30.]\n",
      "[15, 30, 47]\n",
      "[30, 47]\n",
      "[30, 32, 51]\n",
      "That is a potential anomaly prediction: 3 from sequence [51. 30.  3. 30. 30.]\n",
      "[30, 32, 41, 47, 15]\n",
      "[33]\n",
      "[30, 51, 47, 21, 14]\n",
      "[30, 29, 32, 41]\n",
      "That is a potential anomaly prediction: 38 from sequence [30.  3. 30. 33. 30.]\n",
      "[51, 32, 3, 47, 14, 30]\n",
      "[30, 25, 32]\n",
      "That is a potential anomaly prediction: 51 from sequence [30. 33. 30. 38. 30.]\n",
      "[30, 41, 42]\n",
      "[30, 41, 42, 51]\n",
      "[30, 41, 47, 51]\n",
      "[30, 41, 51, 42]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 42 from sequence [51. 41. 42. 30. 51.]\n",
      "[30, 41, 42]\n",
      "[30, 41, 51, 47]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 41 from sequence [30. 51. 42. 41. 41.]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 41 from sequence [51. 42. 41. 41. 41.]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 41 from sequence [42. 41. 41. 41. 41.]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 41 from sequence [41. 41. 41. 41. 41.]\n",
      "[30, 47]\n",
      "[30, 47]\n",
      "[30, 47]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 42 from sequence [41. 41. 30. 30. 30.]\n",
      "[30, 41, 42, 51]\n",
      "[30, 42, 41]\n",
      "[30, 41, 51, 47]\n",
      "[30, 51, 41, 47]\n",
      "[30, 47]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 42 from sequence [42. 42. 30. 30. 30.]\n",
      "[30, 41, 42]\n",
      "[30, 42, 41]\n",
      "[30, 41, 51, 47]\n",
      "[30, 41, 42, 51]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 18 from sequence [42. 42. 42. 41. 41.]\n",
      "[30, 15, 25]\n",
      "That is a potential anomaly prediction: 18 from sequence [42. 42. 41. 41. 18.]\n",
      "[30, 15, 47, 32]\n",
      "[30, 47, 32]\n",
      "[32, 30, 51]\n",
      "That is a potential anomaly prediction: 42 from sequence [41. 18. 18. 30. 30.]\n",
      "[30, 41, 42, 51]\n",
      "[30, 41, 51, 47]\n",
      "[30, 41, 51, 47]\n",
      "[30, 41, 42, 51]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 51 from sequence [42. 42. 41. 41. 30.]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 42 from sequence [42. 41. 41. 30. 51.]\n",
      "[30, 41, 42]\n",
      "That is a potential anomaly prediction: 18 from sequence [41. 41. 30. 51. 42.]\n",
      "[15, 30]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 3 from sequence [30. 51. 42. 18. 30.]\n",
      "[20]\n",
      "That is a potential anomaly prediction: 42 from sequence [51. 42. 18. 30.  3.]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 41 from sequence [42. 18. 30.  3. 42.]\n",
      "[30, 21, 14, 47]\n",
      "[30, 15, 41]\n",
      "That is a potential anomaly prediction: 51 from sequence [30.  3. 42. 41. 30.]\n",
      "[30, 15, 51, 47]\n",
      "That is a potential anomaly prediction: 42 from sequence [ 3. 42. 41. 30. 51.]\n",
      "[30, 41, 42]\n",
      "[15, 30, 3, 51, 47]\n",
      "That is a potential anomaly prediction: 34 from sequence [41. 30. 51. 42. 30.]\n",
      "[30, 47]\n",
      "[15, 30, 47]\n",
      "[30, 47, 15, 32]\n",
      "[30, 25, 32]\n",
      "[30, 25, 32]\n",
      "[30, 25, 32]\n",
      "[30, 25, 32]\n",
      "That is a potential anomaly prediction: 45 from sequence [30. 30. 30. 30. 30.]\n",
      "[30, 42, 41]\n",
      "[30, 51, 41]\n",
      "[30, 41, 42]\n",
      "That is a potential anomaly prediction: 25 from sequence [30. 30. 45. 30. 51.]\n",
      "[30, 51, 42, 41, 25]\n",
      "That is a potential anomaly prediction: 32 from sequence [30. 45. 30. 51. 25.]\n",
      "[30, 47]\n",
      "That is a potential anomaly prediction: 25 from sequence [45. 30. 51. 25. 32.]\n",
      "                                          log message  \\\n",
      "0   3 18 16:34:33 Cluster observer forked - pid = ...   \n",
      "1   3 18 16:34:33 Initialization successful. Start...   \n",
      "2   3 18 16:34:33 SNM Cluster observer starting (P...   \n",
      "3   3 18 16:34:33 We have 1 configured nodes in th...   \n",
      "4                 3 18 16:34:33 Cluster node 1 is UP\"   \n",
      "..                                                ...   \n",
      "95  3 18 16:34:37 Process PmProducer is fully func...   \n",
      "96  3 18 16:34:37 Process FmProducer is fully func...   \n",
      "97  3 18 16:34:37 Process opsapilocald01 is fully ...   \n",
      "98  3 18 16:34:37 Process RtpRestServ is fully fun...   \n",
      "99     3 18 16:34:37 ZTS is reachable for SS Process\"   \n",
      "\n",
      "                                              log key parameter value vector  \\\n",
      "0                Cluster observer forked - pid = <*>\"            [0, '1409']   \n",
      "1   Initialization successful. Starting now the su...                    [0]   \n",
      "2         SNM Cluster observer starting (PID <*>)...\"            [0, '1409']   \n",
      "3        We have <*> configured nodes in the cluster\"               [0, '1']   \n",
      "4                             Cluster node <*> is UP\"               [0, '1']   \n",
      "..                                                ...                    ...   \n",
      "95                  Process <*> is fully functional.\"      [0, 'PmProducer']   \n",
      "96                  Process <*> is fully functional.\"      [0, 'FmProducer']   \n",
      "97                  Process <*> is fully functional.\"  [0, 'opsapilocald01']   \n",
      "98                  Process <*> is fully functional.\"     [0, 'RtpRestServ']   \n",
      "99                  ZTS is reachable for <*> Process\"              [0, 'SS']   \n",
      "\n",
      "                              seq_path  path_pred  \n",
      "0   [17.0, 24.0, 36.0, 44.0, 16.0, 31]          0  \n",
      "1   [24.0, 36.0, 44.0, 16.0, 31.0, 30]          0  \n",
      "2   [36.0, 44.0, 16.0, 31.0, 30.0, 30]          0  \n",
      "3   [44.0, 16.0, 31.0, 30.0, 30.0, 43]          1  \n",
      "4   [16.0, 31.0, 30.0, 30.0, 43.0, 30]          0  \n",
      "..                                 ...        ...  \n",
      "95  [30.0, 30.0, 30.0, 30.0, 45.0, 30]          0  \n",
      "96  [30.0, 30.0, 30.0, 45.0, 30.0, 51]          0  \n",
      "97  [30.0, 30.0, 45.0, 30.0, 51.0, 25]          1  \n",
      "98  [30.0, 45.0, 30.0, 51.0, 25.0, 32]          1  \n",
      "99  [45.0, 30.0, 51.0, 25.0, 32.0, 25]          1  \n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-0a0c965e9c4c>:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trace_df[key] = value\n",
      "<ipython-input-11-0a0c965e9c4c>:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trace_df[key] = value\n"
     ]
    }
   ],
   "source": [
    "    print('===========================')\n",
    "    # pprint(rebuild_data(X[:100], normal_result))\n",
    "    print('================================')\n",
    "    # print(anomaly_match(classifier, X[:100], one_hot_y[:100], 2 ))\n",
    "    seq_pre_dict = anomaly_match(classifier, X[:100], one_hot_y[:100], 0.1)\n",
    "    trace_df = df.iloc[:100,]\n",
    "    \n",
    "    new_trace_df = trace_seq_path(trace_df, seq_pre_dict)\n",
    "    \n",
    "    print(new_trace_df)\n",
    "    new_trace_df.to_csv(PurePosixPath('trace_df.csv'))\n",
    "    # train_batch(classifier, X[:100], one_hot_y[:100])\n",
    "    # train_batch(classifier, X[:100], one_hot_y[:100], 0.5, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa13703",
   "metadata": {},
   "source": [
    "# Parameter Anomaly Model (MAIN)\n",
    "\n",
    "please check code run parameter function below first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ad5e29a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12         ['synchronous']\n",
      "13        ['asynchronous']\n",
      "107692     ['synchronous']\n",
      "107693    ['asynchronous']\n",
      "215372     ['synchronous']\n",
      "215373    ['asynchronous']\n",
      "282566     ['synchronous']\n",
      "282567    ['asynchronous']\n",
      "Name: ParameterList, dtype: object\n",
      "8\n",
      "8\n",
      "['[', '[', '[', '[', '[', '[', '[', '[']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-39-147ad93a76cf>:37: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  if pd.Series(new_data[col_ord]).dtype == 'O':\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-147ad93a76cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;31m# reshape 2D to 1D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mnew_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_ord\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_ord\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_ord\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;31m# shift the row to column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "    import regex as re\n",
    "    # load paths\n",
    "    current_path = r'C:\\Users\\ashoaib\\Desktop\\Logs Analysis\\test'\n",
    "    df = pd.read_pickle(PurePosixPath('logs_data.pkl'))\n",
    "    model_file = PurePosixPath(\"para_model.h5\")\n",
    "    scaler_file =  current_path\n",
    "    label_file =  current_path\n",
    "    trace_df = pd.read_csv(PurePosixPath('trace_df.csv'))\n",
    "    \n",
    "    # load testing parameter value vector\n",
    "    eventId = 16\n",
    "    # with categorical data inside\n",
    "    data = df[df['EventTemplate']=='The Trace Reader for <*> tracing was successfully initialized and is fully functional.\"']['ParameterList']\n",
    "    #data = data.apply(lambda x: re.sub(r'[\\[\\]\\']',r'',x))\n",
    "    print(data)\n",
    "    print(data.shape[0])\n",
    "    print(data.shape[0])\n",
    "    # feature engineering for parameter value matrix\n",
    "    col_num = data.shape[0]\n",
    "    \n",
    "    new_data = []\n",
    "\n",
    "    # feature engineering for every single column\n",
    "    for col_ord in range(col_num):\n",
    "        print([row[col_ord] for row in data])\n",
    "        new_data.append([row[col_ord] for row in data])\n",
    "        # replace the missing values\n",
    "        new_data[col_ord] = miss_rep_col(new_data[col_ord])\n",
    "\n",
    "        # create paths to save encoder model\n",
    "        label_encoder_path = Path(label_file).joinpath(str(eventId), str(col_ord) + 'label.save')\n",
    "        \n",
    "        if not Path(label_encoder_path).parent.is_dir():\n",
    "            Path(label_encoder_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # encode categorical labels\n",
    "        if pd.Series(new_data[col_ord]).dtype == 'O':\n",
    "            new_data[col_ord] = lab_enc(new_data[col_ord], label_encoder_path.as_posix())            \n",
    "        \n",
    "        # nomalize the column\n",
    "        # new_data[col_ord] = \n",
    "        # stan_cols(new_data[col_ord], col_ord, eventId, scaler_file)\n",
    "        \n",
    "        # reshape 2D to 1D\n",
    "        new_data[col_ord] = np.reshape(new_data[col_ord],new_data[col_ord].shape[0])\n",
    "    \n",
    "    # shift the row to column   \n",
    "    new_data = np.array(new_data).T\n",
    "    n_steps = 5\n",
    "    X, y = split_data(new_data, n_steps)\n",
    "    \n",
    "    # reshape x to (samples, time steps, features)\n",
    "    train_X = np.array(X).reshape(-1, n_steps, len(data[0]))\n",
    "    # reshape y to (samples, features)\n",
    "    train_y = np.array(y).reshape(-1, len(data[0]))\n",
    "    model = model_build_train(train_X, train_y, model_file)\n",
    "    # model = keras.models.load_model(model_file)\n",
    "    mse_error = model_predict(model, train_X[:50], train_y[:50])\n",
    "    \n",
    "    print(mse_error)\n",
    "    # confidence = 0.99\n",
    "    # print(confidence_interval(confidence, mse_error))\n",
    "    fp_int = 0.97 \n",
    "    tp_int = 0.999\n",
    "    attempts = 10\n",
    "    threshold1, threshold3, seq_pre_dict = anomaly_match(mse_error, fp_int, tp_int, eventId)\n",
    "    # visual_mses(eventId, mse_error, threshold1, threshold3, fp_int, tp_int)\n",
    "    lab_encoder_file = PurePosixPath(\"encoder_new.save\")\n",
    "    trace_df = trace_seq_path(trace_df, seq_pre_dict, eventId, lab_encoder_file)\n",
    "    trace_df.to_csv(PurePosixPath('trace_df_new.csv'),index=False)\n",
    "    steps = 5\n",
    "    train_batch(model, model_file, train_X[:50], train_y[:50], steps, tp_int, attempts)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5c81f",
   "metadata": {},
   "source": [
    "## Parameter Anomaly Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06dce1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    standardization -- same position in the vector\n",
    "    hstack -- stack columns \n",
    "\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler, MinMaxScaler, Normalizer\n",
    "from pathlib import Path, PurePosixPath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from numpy import subtract, square\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras\n",
    "\n",
    "def miss_rep_col(col):\n",
    "    ''' fill the missing data in parameter vector\n",
    "\n",
    "    : param col: the single column in vector\n",
    "    : return: pandas series column without missing data\n",
    "    '''\n",
    "    # check whether is categorical dtype\n",
    "    if pd.Series(col).dtype == 'O':\n",
    "        # replace nan string to None\n",
    "        return pd.Series(col).replace(np.nan, \"None\")\n",
    "    else:\n",
    "        # replace nan integer to 0\n",
    "        return pd.Series(col).replace(np.nan, 0) \n",
    "\n",
    "\n",
    "def lab_enc(cate_col, label_encoder_file):\n",
    "    ''' encode categorical column in parameter vector to numeric data\n",
    "    \n",
    "    : cate_col: the single series column in vector\n",
    "    : label_encoder_file: the path to save label encoder\n",
    "    : return: encoded series numeric column \n",
    "    '''\n",
    "    \n",
    "    if Path(label_encoder_file).is_file():\n",
    "        label_encoder = joblib.load(label_encoder_file)\n",
    "    else:\n",
    "        laber_encoder = LabelEncoder()\n",
    "        # key_log_arr = cate_list_com.values\n",
    "        label_encoder = laber_encoder.fit(cate_col)\n",
    "        # save the encoder for labelling\n",
    "        joblib.dump(label_encoder, label_encoder_file)\n",
    "\n",
    "    label_encode_cate = label_encoder.transform(cate_col)\n",
    "\n",
    "    return label_encode_cate\n",
    "\n",
    "\n",
    "def stan_cols(col, col_ord, eventId, scaler_file):\n",
    "    ''' normalize the matrix based on every column in parameter vector\n",
    "\n",
    "    : param data: the parameter value matrix\n",
    "    : param eventId: the event number in clusters\n",
    "    : param scaler_file: the path to save/load scaler \n",
    "    : param col_ord: the order of col in a vector\n",
    "    : return: normalized matrix\n",
    "    '''\n",
    "    \n",
    "    scaler_path = Path(scaler_file).joinpath(str(eventId), str(col_ord) + 'scaler.save')\n",
    "    \n",
    "    if scaler_path.is_file():\n",
    "        scaler = joblib.load(scaler_path.as_posix())\n",
    "    \n",
    "    else:\n",
    "        scaler = StandardScaler() \n",
    "        # scaler = RobustScaler()\n",
    "        # scaler = Normalizer()\n",
    "        # reshape to 2D from 1D\n",
    "        col = np.array(col).reshape(-1,1)\n",
    "        scaler = scaler.fit(col)\n",
    "    \n",
    "    # standardilize column\n",
    "    sat_col = scaler.transform(col)\n",
    "    \n",
    "    return sat_col\n",
    "\n",
    "\n",
    "def split_data(data, n_steps):\n",
    "    '''\n",
    "\n",
    "    : param data: the matrix for one event cluster\n",
    "    '''\n",
    "    if isinstance(data, np.ndarray):\n",
    "        length = data.shape[0]\n",
    "    else:\n",
    "        length = len(data)\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(length):\n",
    "        # create the end of position\n",
    "        end_ix = i + n_steps\n",
    "        # check whether the index excesses the boundary\n",
    "        if end_ix > data.shape[0] -1:\n",
    "            break\n",
    "\n",
    "        # get the input and output for model\n",
    "        X_seq, Y_seq = data[i: end_ix], data[end_ix]\n",
    "        \n",
    "        # avoid arrays in a array\n",
    "        X.append(X_seq.tolist())\n",
    "        y.append(Y_seq.tolist())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def model_build_train(train_X, train_y, model_file):\n",
    "    '''\n",
    "        the step is default one\n",
    "    '''\n",
    "    earlystopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(8,activation='relu',input_shape = (train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(8,activation='relu'))\n",
    "    model.add(Dense(train_y.shape[1]))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit model with validation\n",
    "    model.fit(train_X, train_y, epochs=500, batch_size=16, callbacks = [earlystopping], validation_split=0.3, verbose=2, shuffle=False)\n",
    "    \n",
    "    # saving weights\n",
    "    model.save(model_file.as_posix())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def mean_square_error(y_true, y_pred):\n",
    "    ''' modified mse to compute squared error for parameter model evaluation\n",
    "\n",
    "    :param y_true: the test y --- array\n",
    "    :param y_pred: the predict y --- array\n",
    "    :return: the mean of errors, the errors list\n",
    "    '''\n",
    "    # define the minus between two values\n",
    "    # return original value\n",
    "    d_matrix = subtract(y_true, y_pred)\n",
    "    mses = []\n",
    "    print('The shape of minus matrix is: {}'.format(d_matrix.shape))\n",
    "    # compute mse for every row\n",
    "    for i in range(d_matrix.shape[0]):\n",
    "        # initialize to 0 for every new row\n",
    "        sum_minus = 0\n",
    "        for j in range(d_matrix.shape[1]):\n",
    "            sum_minus += d_matrix[i, j] * d_matrix[i, j]\n",
    "        # compute the mse for every row\n",
    "        mse = np.mean(sum_minus)\n",
    "        mses.append(mse)\n",
    "    return mses\n",
    "\n",
    "\n",
    "def model_predict(model, test_x, test_y):\n",
    "    ''' \n",
    "\n",
    "    '''\n",
    "    pre_y = model.predict(test_x, verbose=1)\n",
    "\n",
    "    return mean_square_error(test_y, pre_y)\n",
    "\n",
    "\n",
    "def confidence_interval(confidence, mse):\n",
    "    ''' function to compute the confidence interval boundaries\n",
    "\n",
    "    :param confidence: the confidence value or threshold, like 98%\n",
    "    :param mses_list: the errors list\n",
    "    :return: the boundaries\n",
    "    '''\n",
    "    # define the interval tuple\n",
    "    return st.t.interval(confidence, len(mse)-1, loc=np.mean(mse), scale=st.sem(mse))\n",
    "\n",
    "\n",
    "def anomaly_match(mses_list, fp_int, tp_int, eventId):\n",
    "    '''\n",
    "    : param mses_list: the list of mean square errors\n",
    "    : param file_number: the matrix order\n",
    "    : return: two thresholds (for false positive, true positive),\n",
    "             the indexes of anomaly logs and false positive logs\n",
    "    '''\n",
    "    # here we use the max value as the threshold\n",
    "    CI_fp1 = confidence_interval(fp_int, mses_list)\n",
    "    \n",
    "    # it is for the false positive detection\n",
    "    threshold1 = CI_fp1[1]\n",
    "    \n",
    "    CI_an = confidence_interval(tp_int, mses_list)\n",
    "    # save the result from prediction, index is the order in event matrix\n",
    "    seq_pre_dict = {'seq_para':[],'para_pred':[]}\n",
    "    # it is for the anomaly detection\n",
    "    threshold2 = CI_an[1]\n",
    "\n",
    "    print('[+] Reporting based on thresholds to match anomaly for Parameter Vector Model!')\n",
    "    \n",
    "    for i in range(len(mses_list)):\n",
    "        seq_pre_dict['seq_para'].append(i)\n",
    "        # default add 0 as normal\n",
    "        seq_pre_dict['para_pred'].append(0)\n",
    "        # compare the true positive predictions\n",
    "        if mses_list[i] > threshold2:\n",
    "            print('The {}th log in event {} sequence is potentially anomaly'.format(i, eventId))\n",
    "            seq_pre_dict['para_pred'][-1] = 1\n",
    "        # compare the false positive predictions\n",
    "        elif mses_list[i] > threshold1:\n",
    "            print('The {}th log in event {} sequence is false positive'.format(i, eventId))\n",
    "            seq_pre_dict['para_pred'][-1] = 2\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return threshold1, threshold2, seq_pre_dict\n",
    "\n",
    "\n",
    "def visual_mses(eventId, mses, threshold1, threshold2, CI1, CI2):\n",
    "    ''' visualize the mse\n",
    "\n",
    "    '''\n",
    "    # create the x axis labels\n",
    "    x_list = []\n",
    "    for i in range(len(mses)):\n",
    "        x_list.append(i)\n",
    "    if len(x_list) < 1:\n",
    "        return\n",
    "    else:\n",
    "        plt.plot(x_list, mses)\n",
    "        # add the threshold lines with percentage\n",
    "        plt.axhline(y=threshold1, color='b', linestyle=\"-\", label='CI={}'.format(CI1))\n",
    "        plt.axhline(y=threshold2, color='r', linestyle=\"-\", label='CI={}'.format(CI2))\n",
    "        plt.ylabel(\"Errors Values\")\n",
    "        # match the first num\n",
    "        plt.title('Event '+ str(eventId) + ' ' + 'Errors Distribution')\n",
    "        plt.legend()\n",
    "        plt.show(block=False)\n",
    "        plt.pause(3)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def trace_seq_path(trace_df, seq_pre_dict, eventId, lab_encoder_file):\n",
    "    ''' generate dataframe to view the prediction\n",
    "\n",
    "    : param trace_df: the dataframe with numeric log key, record_id inside        \n",
    "    '''\n",
    "    # check whether the para_pred column has existed or not\n",
    "    if 'para_pred' not in trace_df:\n",
    "        # default assign 0\n",
    "        trace_df['para_pred'] = 0\n",
    "\n",
    "    lab_encoder = joblib.load(lab_encoder_file.as_posix())\n",
    "    # {eventId: log message}\n",
    "\n",
    "    event_log_map = dict(zip(lab_encoder.transform(lab_encoder.classes_), lab_encoder.classes_))\n",
    "    # extract the original log message indexes\n",
    "    ori_log = event_log_map[eventId]\n",
    "    eventId_indexes = trace_df[trace_df['log key'] == ori_log].index\n",
    "    \n",
    "    assert len(eventId_indexes) != len(seq_pre_dict['para_pred']), \" Length Not Matched \"\n",
    "    \n",
    "    for i, index in enumerate(eventId_indexes):\n",
    "        # replace the order with index\n",
    "        seq_pre_dict['seq_para'][i] = int(index)\n",
    "    \n",
    "    for ord_index, df_index in enumerate(seq_pre_dict['seq_para']):\n",
    "        trace_df['para_pred'][df_index] = seq_pre_dict['para_pred'][ord_index]\n",
    "\n",
    "    return trace_df\n",
    "\n",
    "\n",
    "def train_batch(para_model, model_file, batch_x, batch_y, steps, desired_thres, attempts):\n",
    "    ''' update model with false positve and corrected wrong prediction\n",
    "        stop train when predicted mes is smaller than a threshold or attempts reach a given num\n",
    "\n",
    "    : param para_model: the original trained model\n",
    "    : param batch_x: the x used to update the model\n",
    "    : param batch_y: the normal prediction\n",
    "    : param desired_thres: the threshold of confidence interval to match normal prediction\n",
    "    : param attempts: the threshold to stop the training\n",
    "    \n",
    "    : return: updated model with adjusted weights\n",
    "    '''\n",
    "    # train with batch data first\n",
    "    para_model.train_on_batch(batch_x, batch_y)\n",
    "    # check the predict result\n",
    "    mse_error = model_predict(para_model, batch_x, batch_y)\n",
    "    # calculate the value matched the desired threshold for CI\n",
    "    CI_AN = confidence_interval(desired_thres, mse_error)\n",
    "    # compare every mse with the CI_AN\n",
    "    for i in range(len(mse_error)):\n",
    "        # set the exit condition\n",
    "        success_flag = False\n",
    "        no_of_attempts = 0\n",
    "        # retrain if the mse is not acceptable\n",
    "        while mse_error[i] > CI_AN[1] and (no_of_attempts < attempts):\n",
    "            # convert 2D to 3D (samples, time steps, features)\n",
    "            batch_x_one = np.reshape(batch_x[i], (1,batch_x[i].shape[0], batch_x[i].shape[1]))\n",
    "            # convert 1D to 2D\n",
    "            batch_y_one = np.reshape(batch_y[i], (1, len(batch_y[i])))\n",
    "            para_model.fit(batch_x_one, batch_y_one)\n",
    "            \n",
    "            no_of_attempts += 1\n",
    "            mse_one = model_predict(para_model, batch_x_one, batch_y_one)\n",
    "            print(\"Attempt Number %d, Calculated error for this iteration %f\" %(no_of_attempts, mse_one[0]))\n",
    "\n",
    "            if mse_one < CI_AN[1]:\n",
    "                success_flag = True\n",
    "                break\n",
    "\n",
    "        if (success_flag == False) and (no_of_attempts >= attempts):\n",
    "            print(\"[-] Failed to incorporate this feedback\")\n",
    "\n",
    "        if success_flag == True:\n",
    "            print(\"[+] Feedback incorporated \\n\")\n",
    "            print(\"Took %d iterations to learn!\" %(no_of_attempts))\n",
    "\n",
    "    # saving weights\n",
    "    para_model.save(model_file.as_posix())\n",
    "\n",
    "    return para_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4121cc95",
   "metadata": {},
   "source": [
    "## Execution Path Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da9a45e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def miss_rep_col(column_list):\n",
    "    #if column_list.dtype.name != 'category':\n",
    "        # replace nan string to None\n",
    "    #    return column_list.replace(np.nan, \"None\")\n",
    "    #else:\n",
    "        # replace nan integer to 0\n",
    "    #return column_list.replace(np.nan, 0) \n",
    "    pass\n",
    "\n",
    "def lab_enc(cate_list, label_encoder_file):\n",
    "    # feature engineering for list\n",
    "    cate_list_com = miss_rep_col(cate_list)\n",
    "    \n",
    "    if Path(label_encoder_file).is_file():\n",
    "        label_encoder = joblib.load(label_encoder_file)\n",
    "    else:\n",
    "        laber_encoder = LabelEncoder()\n",
    "        # key_log_arr = cate_list_com.values\n",
    "        label_encoder = laber_encoder.fit(cate_list_com)\n",
    "        # save the encoder for labelling\n",
    "        joblib.dump(label_encoder, label_encoder_file.as_posix())\n",
    "    \n",
    "    label_encode_cate = label_encoder.transform(cate_list_com)\n",
    "\n",
    "    return label_encode_cate\n",
    "\n",
    "\n",
    "def encode_key(key_log_series, label_encoder_file, n_steps):\n",
    "    ''' encode the string log key and one hot encode the number then\n",
    "\n",
    "    '''\n",
    "    label_encode_cate = lab_enc(key_log_series, label_encoder_file)\n",
    "    # load label_encoder to return number of classes\n",
    "    label_encoder = joblib.load(label_encoder_file)\n",
    "    class_num = len(label_encoder.classes_)\n",
    "    \n",
    "    X, y = split_data(label_encode_cate, n_steps)\n",
    "\n",
    "    # one hot encoding the labelled numeric log key array\n",
    "    one_hot_y = utils.to_categorical(y) # np_utils\n",
    "\n",
    "    X = np.array(X).astype(float)\n",
    "    \n",
    "    return X , one_hot_y, class_num\n",
    "\n",
    "\n",
    "def model_build(steps, class_num):\n",
    "    ''' build the model with two hidden layers\n",
    "    \n",
    "    : return: compiled model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    # input dim is the length of steps/history\n",
    "    model.add(Dense(16, input_dim=steps, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    # output unit is the number of classes\n",
    "    model.add(Dense(class_num,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def split_data(key_log_list, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(key_log_list)):\n",
    "        # create the end of position\n",
    "        end_ix = i + n_steps\n",
    "        # check whether the index excesses the boundary\n",
    "        if end_ix > len(key_log_list) -1:\n",
    "            break\n",
    "\n",
    "        # get the input and output for model\n",
    "        X_seq, Y_seq = key_log_list[i: end_ix], key_log_list[end_ix]\n",
    "        # avoid arrays in a array\n",
    "        X.append(X_seq.tolist())\n",
    "        y.append(Y_seq.tolist())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fit_eval(model, model_file, random_seed, X, y):\n",
    "    \n",
    "    if Path(model_file).is_file():\n",
    "        model = load_model(model_file.as_posix())# fit the model\n",
    "    else:\n",
    "        print(X)\n",
    "        earlystopping = EarlyStopping(monitor='accuracy', patience=10)\n",
    "        \n",
    "\n",
    "        # classifier = KerasClassifier(model, epochs=200, batch_size =5,  verbose=0)\n",
    "        # evaluate the model\n",
    "        # kfold = KFold(n_splits=10, shuffle=True, random_state=random_seed)\n",
    "        print('=================')\n",
    "        print(y)\n",
    "        # results = cross_val_score(classifier, X, y, cv=kfold)\n",
    "        # print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "        # in order to save model --- use fit\n",
    "        model.fit(X, y, epochs=50, batch_size =5, validation_split =0.2, callbacks = [earlystopping], verbose=2)\n",
    "    \n",
    "        # saving weights\n",
    "        model.save(model_file.as_posix())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_sort(classifier, test_x, top_num):\n",
    "    '''\n",
    "    : param top_num: the top possibility to set as normal\n",
    "    '''\n",
    "    normal_result = []\n",
    "    results = classifier.predict_proba(test_x, batch_size=16, verbose=1)\n",
    "    # sort the result with shape of test_x.shape\n",
    "    ## iterate the rows\n",
    "    for row in range(results.shape[0]):\n",
    "        # sort descending\n",
    "        sorted_poss = [i for i in sorted(enumerate(results[row]), key=lambda x:x[1], reverse=True)]\n",
    "        # extract the top num of possibilities tuple as the normal (class_num, possibility)\n",
    "        normal_result.append(sorted_poss[: top_num])\n",
    "    \n",
    "    return normal_result\n",
    "\n",
    "\n",
    "def rebuild_data(test_x, normal_result):\n",
    "    ''' build normal sequences for updating \n",
    "\n",
    "    : param normal_result: checked predicted normal y -- list type with poss tuples inside\n",
    "    '''\n",
    "    assert test_x.shape[0] != len(normal_result), \"Array Shape does not Match\"\n",
    "    # build normal sequence --- kind of N-gram\n",
    "    normal_sequence = []\n",
    "    \n",
    "    for i in range(test_x.shape[0]):\n",
    "        test_seq = test_x[i].tolist()\n",
    "        normal_sequence.extend([ test_seq + [float(pro_y[0])]  for pro_y in normal_result[i]])\n",
    "\n",
    "    return normal_sequence\n",
    "\n",
    "    pprint(normal_sequence)\n",
    "\n",
    "\n",
    "def anomaly_match(classifier, test_x, test_y, threshold):\n",
    "    ''' match the top normal predict, anomaly logs if not matchable\n",
    "    \n",
    "\n",
    "    '''\n",
    "    # pre_y is a list of predicted class number\n",
    "    pre_y = classifier.predict_classes(test_x, batch_size=16, verbose=1)\n",
    "    # save the possible prediction\n",
    "    normal_result = []\n",
    "    # save the seq and the prediction\n",
    "    seq_pre_dict = {'seq_path':[], 'path_pred':[]}\n",
    "    results = classifier.predict_proba(test_x, batch_size=16, verbose=1)\n",
    "    # sort the result with shape of test_x.shape\n",
    "    ## iterate the rows\n",
    "    for row in range(results.shape[0]):\n",
    "        # sort descending\n",
    "        sorted_poss = [i for i in sorted(enumerate(results[row]), key=lambda x:x[1], reverse=True)]\n",
    "        # pprint(sorted_poss)\n",
    "        # print(sorted_poss)\n",
    "        # extract the top num of possibilities tuple as the normal (class_num, possibility)\n",
    "        pprint([clus_prob for clus_prob in sorted_poss if clus_prob[1] >= threshold])\n",
    "        normal_result.append([clus_prob for clus_prob in sorted_poss if clus_prob[1] >= threshold])\n",
    "\n",
    "    for i in range(len(pre_y)):\n",
    "        # default add 0 as the prediction result to result column\n",
    "        seq_pre_dict['seq_path'].append(list(test_x[i]) + [np.argmax(test_y[i])])\n",
    "        # 0 is the normal\n",
    "        seq_pre_dict['path_pred'].append(0)\n",
    "        # print(np.argmax(test_y[i]))\n",
    "        pprint([pre_y[0] for pre_y in normal_result[i]])\n",
    "        if np.argmax(test_y[i]) not in [pre_y[0] for pre_y in normal_result[i]]:\n",
    "            # change the prediction to anomaly with label 1\n",
    "            seq_pre_dict['path_pred'][-1] = 1\n",
    "            print(\"That is a potential anomaly prediction: {} from sequence {}\".format(np.argmax(test_y[i]), test_x[i]))\n",
    "\n",
    "    return seq_pre_dict    \n",
    "\n",
    "\n",
    "def trace_seq_path(trace_df, seq_pre_dict):\n",
    "    ''' generate dataframe to view the prediction\n",
    "\n",
    "    : param trace_df: the dataframe with numeric log key, record_id inside        \n",
    "    '''\n",
    "    for key, value in seq_pre_dict.items():\n",
    "        assert len(trace_df) == len(value)\n",
    "        trace_df[key] = value\n",
    "\n",
    "    return trace_df\n",
    "\n",
    "\n",
    "def train_batch(exec_model, model_file, batch_x, batch_y, desired_proba, attempts):\n",
    "    ''' update model with false positve and corrected wrong prediction\n",
    "        stop train when predicted proba larger than a given value or attempts reach a given value\n",
    "\n",
    "    : param exec_model: the original trained model\n",
    "    : param batch_x: the x used to update the model\n",
    "    : param batch_y: the normal or corrected y to update the prediction\n",
    "    : param desired_proba: the threshold to stop the train_on_batch\n",
    "    : param attempts: the threshold to stop the training\n",
    "    \n",
    "    : return: updated model with adjusted weights\n",
    "    '''\n",
    "    # train with batch data first\n",
    "    exec_model.train_on_batch(batch_x, batch_y)\n",
    "    # check the predict result\n",
    "    pred_y = exec_model.predict_proba(batch_x, verbose=2)\n",
    "    \n",
    "    for i in range(len(batch_y)):\n",
    "        # extract the predict proba for batch_y\n",
    "        pre_proba = pred_y[i][int(np.argmax(batch_y[i]))]\n",
    "        # set the exit condition\n",
    "        success_flag = False\n",
    "        no_of_attempts = 0\n",
    "        # retrain on the single input and output\n",
    "        while pre_proba <= desired_proba and (no_of_attempts<attempts):\n",
    "            print(pre_proba)            \n",
    "            exec_model.fit(np.reshape(batch_x[i],(1,-1)), np.reshape(batch_y[i],(1,-1)))\n",
    "            \n",
    "            no_of_attempts += 1\n",
    "\n",
    "            pred_one_y = exec_model.predict_proba(np.reshape(batch_x[i],(1,-1)), verbose=2)\n",
    "            pre_proba = pred_one_y[0][int(np.argmax(batch_y[i]))]\n",
    "            \n",
    "            print(\"Attempt Number %d, Predicted Proba for this iteration %f\" %(no_of_attempts, pre_proba))\n",
    "\n",
    "            if pre_proba > desired_proba:\n",
    "                success_flag = True\n",
    "                break\n",
    "\n",
    "        if (success_flag == False) and (no_of_attempts >= attempts):\n",
    "            print(\"[-] Failed to incorporate this feedback\")\n",
    "\n",
    "        if success_flag == True:\n",
    "            print(\"[+] Feedback incorporated \\n\")\n",
    "            print(\"Took %d iterations to learn!\" %(no_of_attempts))\n",
    "\n",
    "    # saving weights\n",
    "    exec_model.save(model_file.as_posix())\n",
    "\n",
    "    return exec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57329dea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
