{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "594dcd87",
   "metadata": {},
   "source": [
    "# Execution Path Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06c8746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Hadoop_2k.log_structured.csv')\n",
    "df.to_pickle('Hadoop_2k.log_structured.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e3843426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from tensorflow.keras import utils\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import joblib\n",
    "from pathlib import Path, PurePosixPath\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbf4825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import np_utils\n",
    "#current_path = Path(__file__).resolve().parent\n",
    "df = pd.read_pickle(PurePosixPath('log_vale_vector_normal.pkl'))\n",
    "encoder_file = PurePosixPath(\"encoder.save\")\n",
    "model_file = PurePosixPath(\"model.h5\")\n",
    "\n",
    "X, one_hot_y, class_num = encode_key(df['log key'], encoder_file, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "efee71aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_build(5, class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "92acc0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17. 30. 44. 82. 54.]\n",
      " [30. 44. 82. 54. 53.]\n",
      " [44. 82. 54. 53. 64.]\n",
      " ...\n",
      " [ 8. 32.  8.  8. 32.]\n",
      " [32.  8.  8. 32. 66.]\n",
      " [ 8.  8. 32. 66. 25.]]\n",
      "=================\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Epoch 1/500\n",
      "320/320 - 0s - loss: 4.5077 - accuracy: 0.3089 - val_loss: 1.3978 - val_accuracy: 0.8571\n",
      "Epoch 2/500\n",
      "320/320 - 0s - loss: 2.5373 - accuracy: 0.5363 - val_loss: 0.5347 - val_accuracy: 1.0000\n",
      "Epoch 3/500\n",
      "320/320 - 0s - loss: 2.1972 - accuracy: 0.5476 - val_loss: 0.3280 - val_accuracy: 1.0000\n",
      "Epoch 4/500\n",
      "320/320 - 0s - loss: 2.0152 - accuracy: 0.5545 - val_loss: 0.2658 - val_accuracy: 1.0000\n",
      "Epoch 5/500\n",
      "320/320 - 0s - loss: 1.8947 - accuracy: 0.5652 - val_loss: 0.2229 - val_accuracy: 1.0000\n",
      "Epoch 6/500\n",
      "320/320 - 0s - loss: 1.8014 - accuracy: 0.5789 - val_loss: 0.1905 - val_accuracy: 1.0000\n",
      "Epoch 7/500\n",
      "320/320 - 0s - loss: 1.7265 - accuracy: 0.5808 - val_loss: 0.1501 - val_accuracy: 1.0000\n",
      "Epoch 8/500\n",
      "320/320 - 0s - loss: 1.6564 - accuracy: 0.5871 - val_loss: 0.1046 - val_accuracy: 1.0000\n",
      "Epoch 9/500\n",
      "320/320 - 0s - loss: 1.6083 - accuracy: 0.5871 - val_loss: 0.1091 - val_accuracy: 1.0000\n",
      "Epoch 10/500\n",
      "320/320 - 0s - loss: 1.5643 - accuracy: 0.5921 - val_loss: 0.1086 - val_accuracy: 1.0000\n",
      "Epoch 11/500\n",
      "320/320 - 0s - loss: 1.5149 - accuracy: 0.5959 - val_loss: 0.1075 - val_accuracy: 1.0000\n",
      "Epoch 12/500\n",
      "320/320 - 0s - loss: 1.4858 - accuracy: 0.6053 - val_loss: 0.0921 - val_accuracy: 1.0000\n",
      "Epoch 13/500\n",
      "320/320 - 0s - loss: 1.4533 - accuracy: 0.6009 - val_loss: 0.0713 - val_accuracy: 1.0000\n",
      "Epoch 14/500\n",
      "320/320 - 0s - loss: 1.4282 - accuracy: 0.6046 - val_loss: 0.0703 - val_accuracy: 1.0000\n",
      "Epoch 15/500\n",
      "320/320 - 0s - loss: 1.3961 - accuracy: 0.6140 - val_loss: 0.0724 - val_accuracy: 1.0000\n",
      "Epoch 16/500\n",
      "320/320 - 0s - loss: 1.3672 - accuracy: 0.6234 - val_loss: 0.0664 - val_accuracy: 1.0000\n",
      "Epoch 17/500\n",
      "320/320 - 0s - loss: 1.3353 - accuracy: 0.6209 - val_loss: 0.0852 - val_accuracy: 1.0000\n",
      "Epoch 18/500\n",
      "320/320 - 0s - loss: 1.3137 - accuracy: 0.6228 - val_loss: 0.0808 - val_accuracy: 1.0000\n",
      "Epoch 19/500\n",
      "320/320 - 0s - loss: 1.2968 - accuracy: 0.6222 - val_loss: 0.0684 - val_accuracy: 1.0000\n",
      "Epoch 20/500\n",
      "320/320 - 0s - loss: 1.2848 - accuracy: 0.6266 - val_loss: 0.0705 - val_accuracy: 1.0000\n",
      "Epoch 21/500\n",
      "320/320 - 0s - loss: 1.2664 - accuracy: 0.6360 - val_loss: 0.0620 - val_accuracy: 1.0000\n",
      "Epoch 22/500\n",
      "320/320 - 0s - loss: 1.2530 - accuracy: 0.6259 - val_loss: 0.0717 - val_accuracy: 1.0000\n",
      "Epoch 23/500\n",
      "320/320 - 0s - loss: 1.2320 - accuracy: 0.6378 - val_loss: 0.0514 - val_accuracy: 1.0000\n",
      "Epoch 24/500\n",
      "320/320 - 0s - loss: 1.2244 - accuracy: 0.6347 - val_loss: 0.0909 - val_accuracy: 1.0000\n",
      "Epoch 25/500\n",
      "320/320 - 0s - loss: 1.2191 - accuracy: 0.6391 - val_loss: 0.0309 - val_accuracy: 1.0000\n",
      "Epoch 26/500\n",
      "320/320 - 0s - loss: 1.1993 - accuracy: 0.6353 - val_loss: 0.0460 - val_accuracy: 1.0000\n",
      "Epoch 27/500\n",
      "320/320 - 0s - loss: 1.1878 - accuracy: 0.6454 - val_loss: 0.0468 - val_accuracy: 1.0000\n",
      "Epoch 28/500\n",
      "320/320 - 0s - loss: 1.1810 - accuracy: 0.6416 - val_loss: 0.0482 - val_accuracy: 1.0000\n",
      "Epoch 29/500\n",
      "320/320 - 0s - loss: 1.1648 - accuracy: 0.6510 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
      "Epoch 30/500\n",
      "320/320 - 0s - loss: 1.1555 - accuracy: 0.6491 - val_loss: 0.0538 - val_accuracy: 1.0000\n",
      "Epoch 31/500\n",
      "320/320 - 0s - loss: 1.1490 - accuracy: 0.6497 - val_loss: 0.0406 - val_accuracy: 1.0000\n",
      "Epoch 32/500\n",
      "320/320 - 0s - loss: 1.1409 - accuracy: 0.6485 - val_loss: 0.0433 - val_accuracy: 1.0000\n",
      "Epoch 33/500\n",
      "320/320 - 0s - loss: 1.1218 - accuracy: 0.6516 - val_loss: 0.0377 - val_accuracy: 1.0000\n",
      "Epoch 34/500\n",
      "320/320 - 0s - loss: 1.1276 - accuracy: 0.6548 - val_loss: 0.0507 - val_accuracy: 1.0000\n",
      "Epoch 35/500\n",
      "320/320 - 0s - loss: 1.1204 - accuracy: 0.6598 - val_loss: 0.0492 - val_accuracy: 1.0000\n",
      "Epoch 36/500\n",
      "320/320 - 0s - loss: 1.1194 - accuracy: 0.6591 - val_loss: 0.0417 - val_accuracy: 1.0000\n",
      "Epoch 37/500\n",
      "320/320 - 0s - loss: 1.0980 - accuracy: 0.6623 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 38/500\n",
      "320/320 - 0s - loss: 1.0922 - accuracy: 0.6610 - val_loss: 0.0507 - val_accuracy: 1.0000\n",
      "Epoch 39/500\n",
      "320/320 - 0s - loss: 1.0870 - accuracy: 0.6629 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
      "Epoch 40/500\n",
      "320/320 - 0s - loss: 1.0783 - accuracy: 0.6617 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
      "Epoch 41/500\n",
      "320/320 - 0s - loss: 1.0806 - accuracy: 0.6642 - val_loss: 0.0348 - val_accuracy: 1.0000\n",
      "Epoch 42/500\n",
      "320/320 - 0s - loss: 1.0700 - accuracy: 0.6723 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "Epoch 43/500\n",
      "320/320 - 0s - loss: 1.0802 - accuracy: 0.6573 - val_loss: 0.0292 - val_accuracy: 1.0000\n",
      "Epoch 44/500\n",
      "320/320 - 0s - loss: 1.0742 - accuracy: 0.6642 - val_loss: 0.0278 - val_accuracy: 1.0000\n",
      "Epoch 45/500\n",
      "320/320 - 0s - loss: 1.0667 - accuracy: 0.6692 - val_loss: 0.0243 - val_accuracy: 1.0000\n",
      "Epoch 46/500\n",
      "320/320 - 0s - loss: 1.0625 - accuracy: 0.6698 - val_loss: 0.0206 - val_accuracy: 1.0000\n",
      "Epoch 47/500\n",
      "320/320 - 0s - loss: 1.0570 - accuracy: 0.6685 - val_loss: 0.0299 - val_accuracy: 1.0000\n",
      "Epoch 48/500\n",
      "320/320 - 0s - loss: 1.0386 - accuracy: 0.6704 - val_loss: 0.0271 - val_accuracy: 1.0000\n",
      "Epoch 49/500\n",
      "320/320 - 0s - loss: 1.0503 - accuracy: 0.6754 - val_loss: 0.0346 - val_accuracy: 1.0000\n",
      "Epoch 50/500\n",
      "320/320 - 0s - loss: 1.0355 - accuracy: 0.6729 - val_loss: 0.0408 - val_accuracy: 1.0000\n",
      "Epoch 51/500\n",
      "320/320 - 0s - loss: 1.0423 - accuracy: 0.6748 - val_loss: 0.0364 - val_accuracy: 1.0000\n",
      "Epoch 52/500\n",
      "320/320 - 0s - loss: 1.0314 - accuracy: 0.6779 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
      "Epoch 53/500\n",
      "320/320 - 0s - loss: 1.0237 - accuracy: 0.6786 - val_loss: 0.0345 - val_accuracy: 1.0000\n",
      "Epoch 54/500\n",
      "320/320 - 0s - loss: 1.0335 - accuracy: 0.6779 - val_loss: 0.0269 - val_accuracy: 1.0000\n",
      "Epoch 55/500\n",
      "320/320 - 0s - loss: 1.0192 - accuracy: 0.6723 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
      "Epoch 56/500\n",
      "320/320 - 0s - loss: 1.0238 - accuracy: 0.6842 - val_loss: 0.0329 - val_accuracy: 1.0000\n",
      "Epoch 57/500\n",
      "320/320 - 0s - loss: 1.0189 - accuracy: 0.6798 - val_loss: 0.0180 - val_accuracy: 1.0000\n",
      "Epoch 58/500\n",
      "320/320 - 0s - loss: 1.0060 - accuracy: 0.6842 - val_loss: 0.0296 - val_accuracy: 1.0000\n",
      "Epoch 59/500\n",
      "320/320 - 0s - loss: 1.0127 - accuracy: 0.6817 - val_loss: 0.0396 - val_accuracy: 1.0000\n",
      "Epoch 60/500\n",
      "320/320 - 0s - loss: 0.9994 - accuracy: 0.6754 - val_loss: 0.0340 - val_accuracy: 1.0000\n",
      "Epoch 61/500\n",
      "320/320 - 0s - loss: 1.0115 - accuracy: 0.6848 - val_loss: 0.0288 - val_accuracy: 1.0000\n",
      "Epoch 62/500\n",
      "320/320 - 0s - loss: 1.0168 - accuracy: 0.6823 - val_loss: 0.0358 - val_accuracy: 1.0000\n",
      "Epoch 63/500\n",
      "320/320 - 0s - loss: 1.0051 - accuracy: 0.6798 - val_loss: 0.0217 - val_accuracy: 1.0000\n",
      "Epoch 64/500\n",
      "320/320 - 0s - loss: 0.9868 - accuracy: 0.6848 - val_loss: 0.0374 - val_accuracy: 1.0000\n",
      "Epoch 65/500\n",
      "320/320 - 0s - loss: 0.9920 - accuracy: 0.6855 - val_loss: 0.0290 - val_accuracy: 1.0000\n",
      "Epoch 66/500\n",
      "320/320 - 0s - loss: 0.9863 - accuracy: 0.6880 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
      "Epoch 67/500\n",
      "320/320 - 0s - loss: 0.9776 - accuracy: 0.6898 - val_loss: 0.0336 - val_accuracy: 1.0000\n",
      "Epoch 68/500\n",
      "320/320 - 0s - loss: 0.9705 - accuracy: 0.6867 - val_loss: 0.0373 - val_accuracy: 1.0000\n",
      "Epoch 69/500\n",
      "320/320 - 0s - loss: 0.9721 - accuracy: 0.6917 - val_loss: 0.0212 - val_accuracy: 1.0000\n",
      "Epoch 70/500\n",
      "320/320 - 0s - loss: 0.9795 - accuracy: 0.6905 - val_loss: 0.0275 - val_accuracy: 1.0000\n",
      "Epoch 71/500\n",
      "320/320 - 0s - loss: 0.9900 - accuracy: 0.6936 - val_loss: 0.0267 - val_accuracy: 1.0000\n",
      "Epoch 72/500\n",
      "320/320 - 0s - loss: 0.9709 - accuracy: 0.6892 - val_loss: 0.0179 - val_accuracy: 1.0000\n",
      "Epoch 73/500\n",
      "320/320 - 0s - loss: 0.9575 - accuracy: 0.6905 - val_loss: 0.0441 - val_accuracy: 1.0000\n",
      "Epoch 74/500\n",
      "320/320 - 0s - loss: 0.9583 - accuracy: 0.6898 - val_loss: 0.0353 - val_accuracy: 1.0000\n",
      "Epoch 75/500\n",
      "320/320 - 0s - loss: 0.9717 - accuracy: 0.6886 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
      "Epoch 76/500\n",
      "320/320 - 0s - loss: 0.9759 - accuracy: 0.6892 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
      "Epoch 77/500\n",
      "320/320 - 0s - loss: 0.9581 - accuracy: 0.7036 - val_loss: 0.0213 - val_accuracy: 1.0000\n",
      "Epoch 78/500\n",
      "320/320 - 0s - loss: 0.9597 - accuracy: 0.6898 - val_loss: 0.0197 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/500\n",
      "320/320 - 0s - loss: 0.9511 - accuracy: 0.6942 - val_loss: 0.0308 - val_accuracy: 1.0000\n",
      "Epoch 80/500\n",
      "320/320 - 0s - loss: 0.9482 - accuracy: 0.6905 - val_loss: 0.0357 - val_accuracy: 1.0000\n",
      "Epoch 81/500\n",
      "320/320 - 0s - loss: 0.9633 - accuracy: 0.6949 - val_loss: 0.0258 - val_accuracy: 1.0000\n",
      "Epoch 82/500\n",
      "320/320 - 0s - loss: 0.9566 - accuracy: 0.6986 - val_loss: 0.0291 - val_accuracy: 1.0000\n",
      "Epoch 83/500\n",
      "320/320 - 0s - loss: 0.9468 - accuracy: 0.6892 - val_loss: 0.0202 - val_accuracy: 1.0000\n",
      "Epoch 84/500\n",
      "320/320 - 0s - loss: 0.9596 - accuracy: 0.6924 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
      "Epoch 85/500\n",
      "320/320 - 0s - loss: 0.9346 - accuracy: 0.7011 - val_loss: 0.0188 - val_accuracy: 1.0000\n",
      "Epoch 86/500\n",
      "320/320 - 0s - loss: 0.9461 - accuracy: 0.6924 - val_loss: 0.0256 - val_accuracy: 1.0000\n",
      "Epoch 87/500\n",
      "320/320 - 0s - loss: 0.9323 - accuracy: 0.7018 - val_loss: 0.0257 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "classifier = fit_eval(model, model_file ,30, X, one_hot_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b8188066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================\n",
      "================================\n",
      "7/7 [==============================] - 0s 857us/step\n",
      "7/7 [==============================] - 0s 568us/step\n",
      "[(53, 0.56176656), (48, 0.13837843), (8, 0.13103008)]\n",
      "[(99, 0.19114694), (65, 0.14570843), (86, 0.106979854)]\n",
      "[(61, 0.18401375), (86, 0.15346241), (69, 0.13978109)]\n",
      "[(86, 0.33026534), (57, 0.2949015)]\n",
      "[(57, 0.331252), (61, 0.26409027), (59, 0.1709421)]\n",
      "[(57, 0.2801828), (61, 0.24373235), (59, 0.19394921), (86, 0.12069849)]\n",
      "[(57, 0.33297423), (61, 0.31140238), (59, 0.25444043)]\n",
      "[(57, 0.33297423), (61, 0.31140238), (59, 0.25444043)]\n",
      "[(57, 0.33297423), (61, 0.31140238), (59, 0.25444043)]\n",
      "[(57, 0.33297423), (61, 0.31140238), (59, 0.25444043)]\n",
      "[(59, 0.2020591),\n",
      " (98, 0.15582582),\n",
      " (40, 0.1283086),\n",
      " (32, 0.120795585),\n",
      " (11, 0.10384282)]\n",
      "[(15, 0.3474652), (35, 0.23037107), (59, 0.16703503), (8, 0.10229686)]\n",
      "[(26, 0.66507125), (8, 0.2796345)]\n",
      "[(8, 0.6093672), (9, 0.121703394)]\n",
      "[(92, 0.63840395), (8, 0.10335072)]\n",
      "[(67, 0.79771155), (92, 0.10573802)]\n",
      "[(48, 0.45396122), (59, 0.15363374), (61, 0.13302617)]\n",
      "[(6, 0.28121233), (52, 0.24669082), (99, 0.12735607), (45, 0.11900238)]\n",
      "[(49, 0.9943904)]\n",
      "[(37, 0.7110256), (69, 0.1187142)]\n",
      "[(51, 0.32975957), (57, 0.26667303), (65, 0.10155931)]\n",
      "[(61, 0.21964376), (57, 0.18845806), (8, 0.16878617), (59, 0.13917093)]\n",
      "[(47, 0.46085155), (21, 0.25318876)]\n",
      "[(86, 0.26899794), (65, 0.26642743), (57, 0.12502728)]\n",
      "[(73, 0.34277564), (77, 0.20611617), (61, 0.116153695)]\n",
      "[(7, 0.7271332), (61, 0.12099567)]\n",
      "[(38, 0.7934396), (74, 0.17027606)]\n",
      "[(35, 0.93734795)]\n",
      "[(61, 0.3554064), (8, 0.22977017), (57, 0.12644511), (36, 0.1127876)]\n",
      "[(8, 0.25780773), (57, 0.19849658), (46, 0.15220001)]\n",
      "[(57, 0.25050536), (59, 0.15604874), (61, 0.14458996)]\n",
      "[(57, 0.2735547), (59, 0.15948917), (61, 0.14543486)]\n",
      "[(32, 0.60366344), (3, 0.12411941)]\n",
      "[(3, 0.9107343)]\n",
      "[(85, 0.94849724)]\n",
      "[(85, 0.9547063)]\n",
      "[(42, 0.98690945)]\n",
      "[(90, 0.85065883), (65, 0.12058224)]\n",
      "[(31, 0.62854695), (86, 0.36992133)]\n",
      "[(96, 0.5890209), (72, 0.37797108)]\n",
      "[(61, 0.41970736), (57, 0.15449965), (84, 0.12225339)]\n",
      "[(63, 0.8553691)]\n",
      "[(65, 0.7587267), (86, 0.13716178)]\n",
      "[(86, 0.5866104), (65, 0.11509747)]\n",
      "[(96, 0.7512659), (52, 0.2055544)]\n",
      "[(61, 0.282361), (95, 0.2414606), (57, 0.2144686)]\n",
      "[(57, 0.26167142), (88, 0.18649079), (65, 0.16247751)]\n",
      "[(86, 0.33840233), (73, 0.23358959), (65, 0.15031275), (56, 0.10261696)]\n",
      "[(45, 0.39887774), (86, 0.23580472), (52, 0.15752508)]\n",
      "[(65, 0.6044914), (35, 0.3142783)]\n",
      "[(15, 0.70990086), (65, 0.26667163)]\n",
      "[(94, 0.7258308), (59, 0.13640562)]\n",
      "[(97, 0.4625903), (8, 0.14596014), (23, 0.13497382)]\n",
      "[(80, 0.9722482)]\n",
      "[(101, 0.76134175)]\n",
      "[(65, 0.51974595), (86, 0.4371425)]\n",
      "[(59, 0.3328551), (61, 0.30342838), (57, 0.19740252)]\n",
      "[(52, 0.39252225), (65, 0.20782946), (11, 0.12971935)]\n",
      "[(69, 0.4201473), (65, 0.26062784), (61, 0.118054874), (59, 0.1112915)]\n",
      "[(65, 0.8917895)]\n",
      "[(99, 0.67495173), (61, 0.11759097)]\n",
      "[(65, 0.6036619), (86, 0.19144395)]\n",
      "[(65, 0.48422056), (86, 0.181696)]\n",
      "[(99, 0.7285497)]\n",
      "[(65, 0.7869934)]\n",
      "[(65, 0.7482019), (86, 0.10243955)]\n",
      "[(99, 0.7285497)]\n",
      "[(65, 0.7869934)]\n",
      "[(65, 0.7482019), (86, 0.10243955)]\n",
      "[(99, 0.7285497)]\n",
      "[(65, 0.7869934)]\n",
      "[(65, 0.7482019), (86, 0.10243955)]\n",
      "[(99, 0.7285497)]\n",
      "[(65, 0.7869934)]\n",
      "[(65, 0.7482019), (86, 0.10243955)]\n",
      "[(99, 0.7285497)]\n",
      "[(65, 0.7869934)]\n",
      "[(65, 0.7482019), (86, 0.10243955)]\n",
      "[(99, 0.7285497)]\n",
      "[(65, 0.7869934)]\n",
      "[(65, 0.7482019), (86, 0.10243955)]\n",
      "[(99, 0.7285497)]\n",
      "[(65, 0.7869934)]\n",
      "[(65, 0.7482019), (86, 0.10243955)]\n",
      "[(99, 0.7285497)]\n",
      "[(65, 0.7869934)]\n",
      "[(65, 0.7482019), (86, 0.10243955)]\n",
      "[(99, 0.7285497)]\n",
      "[(65, 0.7869934)]\n",
      "[(1, 0.36659014), (65, 0.34777486), (86, 0.25533158)]\n",
      "[(86, 0.73846745)]\n",
      "[(65, 0.8236972), (86, 0.1192247)]\n",
      "[(59, 0.31030923), (61, 0.2904086), (65, 0.18521887), (57, 0.13349128)]\n",
      "[(61, 0.41917655), (59, 0.2823364), (57, 0.2717056)]\n",
      "[(61, 0.3766889), (57, 0.27315152), (59, 0.24563079)]\n",
      "[(61, 0.3766889), (57, 0.27315152), (59, 0.24563079)]\n",
      "[(61, 0.3766889), (57, 0.27315152), (59, 0.24563079)]\n",
      "[(61, 0.3766889), (57, 0.27315152), (59, 0.24563079)]\n",
      "[(61, 0.3766889), (57, 0.27315152), (59, 0.24563079)]\n",
      "[(61, 0.3766889), (57, 0.27315152), (59, 0.24563079)]\n",
      "[53, 48, 8]\n",
      "[99, 65, 86]\n",
      "That is a potential anomaly prediction: 64 from sequence [30. 44. 82. 54. 53.]\n",
      "[61, 86, 69]\n",
      "That is a potential anomaly prediction: 64 from sequence [44. 82. 54. 53. 64.]\n",
      "[86, 57]\n",
      "That is a potential anomaly prediction: 64 from sequence [82. 54. 53. 64. 64.]\n",
      "[57, 61, 59]\n",
      "That is a potential anomaly prediction: 64 from sequence [54. 53. 64. 64. 64.]\n",
      "[57, 61, 59, 86]\n",
      "That is a potential anomaly prediction: 64 from sequence [53. 64. 64. 64. 64.]\n",
      "[57, 61, 59]\n",
      "That is a potential anomaly prediction: 64 from sequence [64. 64. 64. 64. 64.]\n",
      "[57, 61, 59]\n",
      "That is a potential anomaly prediction: 64 from sequence [64. 64. 64. 64. 64.]\n",
      "[57, 61, 59]\n",
      "That is a potential anomaly prediction: 64 from sequence [64. 64. 64. 64. 64.]\n",
      "[57, 61, 59]\n",
      "That is a potential anomaly prediction: 20 from sequence [64. 64. 64. 64. 64.]\n",
      "[59, 98, 40, 32, 11]\n",
      "That is a potential anomaly prediction: 20 from sequence [64. 64. 64. 64. 20.]\n",
      "[15, 35, 59, 8]\n",
      "That is a potential anomaly prediction: 20 from sequence [64. 64. 64. 20. 20.]\n",
      "[26, 8]\n",
      "[8, 9]\n",
      "That is a potential anomaly prediction: 64 from sequence [64. 20. 20. 20. 26.]\n",
      "[92, 8]\n",
      "[67, 92]\n",
      "[48, 59, 61]\n",
      "[6, 52, 99, 45]\n",
      "[49]\n",
      "[37, 69]\n",
      "[51, 57, 65]\n",
      "[61, 57, 8, 59]\n",
      "That is a potential anomaly prediction: 91 from sequence [48.  6. 49. 37. 51.]\n",
      "[47, 21]\n",
      "[86, 65, 57]\n",
      "That is a potential anomaly prediction: 81 from sequence [49. 37. 51. 91. 47.]\n",
      "[73, 77, 61]\n",
      "[7, 61]\n",
      "[38, 74]\n",
      "[35]\n",
      "[61, 8, 57, 36]\n",
      "[8, 57, 46]\n",
      "[57, 59, 61]\n",
      "That is a potential anomaly prediction: 34 from sequence [ 7. 38. 35. 36. 46.]\n",
      "[57, 59, 61]\n",
      "That is a potential anomaly prediction: 4 from sequence [38. 35. 36. 46. 34.]\n",
      "[32, 3]\n",
      "[3]\n",
      "[85]\n",
      "[85]\n",
      "[42]\n",
      "[90, 65]\n",
      "[31, 86]\n",
      "[96, 72]\n",
      "[61, 57, 84]\n",
      "[63]\n",
      "[65, 86]\n",
      "That is a potential anomaly prediction: 81 from sequence [90. 31. 72. 84. 63.]\n",
      "[86, 65]\n",
      "That is a potential anomaly prediction: 39 from sequence [31. 72. 84. 63. 81.]\n",
      "[96, 52]\n",
      "[61, 95, 57]\n",
      "[57, 88, 65]\n",
      "[86, 73, 65, 56]\n",
      "[45, 86, 52]\n",
      "That is a potential anomaly prediction: 36 from sequence [39. 96. 95. 88. 73.]\n",
      "[65, 35]\n",
      "[15, 65]\n",
      "[94, 59]\n",
      "[97, 8, 23]\n",
      "[80]\n",
      "[101]\n",
      "[65, 86]\n",
      "That is a potential anomaly prediction: 91 from sequence [ 15.  94.  97.  80. 101.]\n",
      "[59, 61, 57]\n",
      "That is a potential anomaly prediction: 55 from sequence [ 94.  97.  80. 101.  91.]\n",
      "[52, 65, 11]\n",
      "That is a potential anomaly prediction: 91 from sequence [ 97.  80. 101.  91.  55.]\n",
      "[69, 65, 61, 59]\n",
      "[65]\n",
      "[99, 61]\n",
      "[65, 86]\n",
      "[65, 86]\n",
      "[99]\n",
      "[65]\n",
      "[65, 86]\n",
      "[99]\n",
      "[65]\n",
      "[65, 86]\n",
      "[99]\n",
      "[65]\n",
      "[65, 86]\n",
      "[99]\n",
      "[65]\n",
      "[65, 86]\n",
      "[99]\n",
      "[65]\n",
      "[65, 86]\n",
      "[99]\n",
      "[65]\n",
      "[65, 86]\n",
      "[99]\n",
      "[65]\n",
      "[65, 86]\n",
      "[99]\n",
      "[65]\n",
      "[65, 86]\n",
      "[99]\n",
      "[65]\n",
      "That is a potential anomaly prediction: 100 from sequence [65. 99. 65. 65. 99.]\n",
      "[1, 65, 86]\n",
      "[86]\n",
      "[65, 86]\n",
      "[59, 61, 65, 57]\n",
      "That is a potential anomaly prediction: 86 from sequence [ 99. 100.  86.  86.  86.]\n",
      "[61, 59, 57]\n",
      "That is a potential anomaly prediction: 86 from sequence [100.  86.  86.  86.  86.]\n",
      "[61, 57, 59]\n",
      "That is a potential anomaly prediction: 86 from sequence [86. 86. 86. 86. 86.]\n",
      "[61, 57, 59]\n",
      "That is a potential anomaly prediction: 86 from sequence [86. 86. 86. 86. 86.]\n",
      "[61, 57, 59]\n",
      "That is a potential anomaly prediction: 86 from sequence [86. 86. 86. 86. 86.]\n",
      "[61, 57, 59]\n",
      "That is a potential anomaly prediction: 86 from sequence [86. 86. 86. 86. 86.]\n",
      "[61, 57, 59]\n",
      "That is a potential anomaly prediction: 86 from sequence [86. 86. 86. 86. 86.]\n",
      "[61, 57, 59]\n",
      "That is a potential anomaly prediction: 87 from sequence [86. 86. 86. 86. 86.]\n",
      "    Unnamed: 0                                        log message  \\\n",
      "0            0  10 18 18:01:47 Created MRAppMaster for applica...   \n",
      "1            1              10 18 18:01:48 Executing with tokens:   \n",
      "2            2  10 18 18:01:48 Kind: YARN_AM_RM_TOKEN, Service...   \n",
      "3            3       10 18 18:01:49 Using mapred newApiCommitter.   \n",
      "4            4  10 18 18:01:50 OutputCommitter set in config null   \n",
      "..         ...                                                ...   \n",
      "95          95  10 18 18:01:53 attempt_1445144423722_0020_m_00...   \n",
      "96          96  10 18 18:01:53 attempt_1445144423722_0020_m_00...   \n",
      "97          97  10 18 18:01:53 attempt_1445144423722_0020_m_00...   \n",
      "98          98  10 18 18:01:53 attempt_1445144423722_0020_m_00...   \n",
      "99          99  10 18 18:01:53 attempt_1445144423722_0020_m_00...   \n",
      "\n",
      "                                              log key  \\\n",
      "0   Created MRAppMaster for application appattempt...   \n",
      "1                              Executing with tokens:   \n",
      "2   Kind: YARN_AM_RM_TOKEN, Service: , Ident: (app...   \n",
      "3                       Using mapred newApiCommitter.   \n",
      "4                  OutputCommitter set in config null   \n",
      "..                                                ...   \n",
      "95  attempt_<*>_<*>_m_<*>_<*> TaskAttempt Transiti...   \n",
      "96  attempt_<*>_<*>_m_<*>_<*> TaskAttempt Transiti...   \n",
      "97  attempt_<*>_<*>_m_<*>_<*> TaskAttempt Transiti...   \n",
      "98  attempt_<*>_<*>_m_<*>_<*> TaskAttempt Transiti...   \n",
      "99  attempt_<*>_<*>_m_<*>_<*> TaskAttempt Transiti...   \n",
      "\n",
      "                               parameter value vector  \\\n",
      "0              [0, '1445144423722', '0020', '000001']   \n",
      "1                                                 [1]   \n",
      "2       [0, '20', '1445144423722', '1', '-127633188']   \n",
      "3                                                 [1]   \n",
      "4                                                 [1]   \n",
      "..                                                ...   \n",
      "95  [0, '1445144423722', '0020', '000001', '0', 'N...   \n",
      "96  [0, '1445144423722', '0020', '000002', '0', 'N...   \n",
      "97  [0, '1445144423722', '0020', '000003', '0', 'N...   \n",
      "98  [0, '1445144423722', '0020', '000004', '0', 'N...   \n",
      "99  [0, '1445144423722', '0020', '000005', '0', 'N...   \n",
      "\n",
      "                              seq_path  path_pred  \n",
      "0   [17.0, 30.0, 44.0, 82.0, 54.0, 53]          0  \n",
      "1   [30.0, 44.0, 82.0, 54.0, 53.0, 64]          1  \n",
      "2   [44.0, 82.0, 54.0, 53.0, 64.0, 64]          1  \n",
      "3   [82.0, 54.0, 53.0, 64.0, 64.0, 64]          1  \n",
      "4   [54.0, 53.0, 64.0, 64.0, 64.0, 64]          1  \n",
      "..                                 ...        ...  \n",
      "95  [86.0, 86.0, 86.0, 86.0, 86.0, 86]          1  \n",
      "96  [86.0, 86.0, 86.0, 86.0, 86.0, 86]          1  \n",
      "97  [86.0, 86.0, 86.0, 86.0, 86.0, 86]          1  \n",
      "98  [86.0, 86.0, 86.0, 86.0, 86.0, 86]          1  \n",
      "99  [86.0, 86.0, 86.0, 86.0, 86.0, 87]          1  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-48-d762fff0495d>:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trace_df[key] = value\n",
      "<ipython-input-48-d762fff0495d>:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trace_df[key] = value\n"
     ]
    }
   ],
   "source": [
    "    # normal_result = predict_sort(classifier, X[:100], top_num=2)\n",
    "    # pprint(normal_result)\n",
    "    print('===========================')\n",
    "    # pprint(rebuild_data(X[:100], normal_result))\n",
    "    print('================================')\n",
    "    # print(anomaly_match(classifier, X[:100], one_hot_y[:100], 2 ))\n",
    "    seq_pre_dict = anomaly_match(classifier, X[:100], one_hot_y[:100], 0.1)\n",
    "    trace_df = df.iloc[:100,]\n",
    "    \n",
    "    new_trace_df = trace_seq_path(trace_df, seq_pre_dict)\n",
    "    \n",
    "    print(new_trace_df)\n",
    "    new_trace_df.to_csv(PurePosixPath('trace_df.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bb193574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3621\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3622\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-9b411c8f822b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# feature engineering for parameter value matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mcol_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 958\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    959\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m         \u001b[1;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m         \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1070\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3622\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3623\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3624\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3625\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "\n",
    "    # load paths\n",
    "    current_path = r'C:\\Users\\ashoaib\\Desktop\\Logs Analysis\\test'\n",
    "    df = pd.read_pickle(PurePosixPath('Hadoop_2k.log_structured.pkl'))\n",
    "    model_file = PurePosixPath(\"para_model.h5\")\n",
    "    scaler_file =  current_path\n",
    "    label_file =  current_path\n",
    "    trace_df = pd.read_csv(PurePosixPath('trace_df.csv'))\n",
    "    \n",
    "    # load testing parameter value vector\n",
    "    eventId = 16\n",
    "    # with categorical data inside\n",
    "    data = df[df['EventTemplate']=='Address change detected. Old: msra-sa-<*><*> New: msra-sa-<*>:<*>']['ParameterList']\n",
    "    print(data.shape[0])\n",
    "    # feature engineering for parameter value matrix\n",
    "    col_num = len(data[0])\n",
    "    \n",
    "    new_data = []\n",
    "\n",
    "    # feature engineering for every single column\n",
    "    for col_ord in range(col_num):\n",
    "        \n",
    "        new_data.append([row[col_ord] for row in data])\n",
    "        # replace the missing values\n",
    "        new_data[col_ord] = miss_rep_col(new_data[col_ord])\n",
    "\n",
    "        # create paths to save encoder model\n",
    "        label_encoder_path = Path(label_file).joinpath(str(eventId), str(col_ord) + 'label.save')\n",
    "        \n",
    "        if not Path(label_encoder_path).parent.is_dir():\n",
    "            Path(label_encoder_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # encode categorical labels\n",
    "        if pd.Series(new_data[col_ord]).dtype == 'O':\n",
    "            new_data[col_ord] = lab_enc(new_data[col_ord], label_encoder_path.as_posix())            \n",
    "        \n",
    "        # nomalize the column\n",
    "        # new_data[col_ord] = \n",
    "        # stan_cols(new_data[col_ord], col_ord, eventId, scaler_file)\n",
    "        \n",
    "        # reshape 2D to 1D\n",
    "        new_data[col_ord] = np.reshape(new_data[col_ord],new_data[col_ord].shape[0])\n",
    "    \n",
    "    # shift the row to column   \n",
    "    new_data = np.array(new_data).T\n",
    "    n_steps = 5\n",
    "    X, y = split_data(new_data, n_steps)\n",
    "    \n",
    "    # reshape x to (samples, time steps, features)\n",
    "    train_X = np.array(X).reshape(-1, n_steps, len(data[0]))\n",
    "    # reshape y to (samples, features)\n",
    "    train_y = np.array(y).reshape(-1, len(data[0]))\n",
    "    model = model_build_train(train_X, train_y, model_file)\n",
    "    # model = keras.models.load_model(model_file)\n",
    "    mse_error = model_predict(model, train_X[:50], train_y[:50])\n",
    "    \n",
    "    print(mse_error)\n",
    "    # confidence = 0.99\n",
    "    # print(confidence_interval(confidence, mse_error))\n",
    "    fp_int = 0.97 \n",
    "    tp_int = 0.999\n",
    "    attempts = 10\n",
    "    threshold1, threshold3, seq_pre_dict = anomaly_match(mse_error, fp_int, tp_int, eventId)\n",
    "    # visual_mses(eventId, mse_error, threshold1, threshold3, fp_int, tp_int)\n",
    "    lab_encoder_file = PurePosixPath(\"encoder_new.save\")\n",
    "    trace_df = trace_seq_path(trace_df, seq_pre_dict, eventId, lab_encoder_file)\n",
    "    trace_df.to_csv(PurePosixPath('trace_df_new.csv'),index=False)\n",
    "    steps = 5\n",
    "    train_batch(model, model_file, train_X[:50], train_y[:50], steps, tp_int, attempts)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5322ab3",
   "metadata": {},
   "source": [
    "## Parameter Anomaly Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b74a4e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    standardization -- same position in the vector\n",
    "    hstack -- stack columns \n",
    "\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler, MinMaxScaler, Normalizer\n",
    "from pathlib import Path, PurePosixPath\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from numpy import subtract, square\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras\n",
    "\n",
    "def miss_rep_col(col):\n",
    "    ''' fill the missing data in parameter vector\n",
    "\n",
    "    : param col: the single column in vector\n",
    "    : return: pandas series column without missing data\n",
    "    '''\n",
    "    # check whether is categorical dtype\n",
    "    if pd.Series(col).dtype == 'O':\n",
    "        # replace nan string to None\n",
    "        return pd.Series(col).replace(np.nan, \"None\")\n",
    "    else:\n",
    "        # replace nan integer to 0\n",
    "        return pd.Series(col).replace(np.nan, 0) \n",
    "\n",
    "\n",
    "def lab_enc(cate_col, label_encoder_file):\n",
    "    ''' encode categorical column in parameter vector to numeric data\n",
    "    \n",
    "    : cate_col: the single series column in vector\n",
    "    : label_encoder_file: the path to save label encoder\n",
    "    : return: encoded series numeric column \n",
    "    '''\n",
    "    \n",
    "    if Path(label_encoder_file).is_file():\n",
    "        label_encoder = joblib.load(label_encoder_file)\n",
    "    else:\n",
    "        laber_encoder = LabelEncoder()\n",
    "        # key_log_arr = cate_list_com.values\n",
    "        label_encoder = laber_encoder.fit(cate_col)\n",
    "        # save the encoder for labelling\n",
    "        joblib.dump(label_encoder, label_encoder_file)\n",
    "\n",
    "    label_encode_cate = label_encoder.transform(cate_col)\n",
    "\n",
    "    return label_encode_cate\n",
    "\n",
    "\n",
    "def stan_cols(col, col_ord, eventId, scaler_file):\n",
    "    ''' normalize the matrix based on every column in parameter vector\n",
    "\n",
    "    : param data: the parameter value matrix\n",
    "    : param eventId: the event number in clusters\n",
    "    : param scaler_file: the path to save/load scaler \n",
    "    : param col_ord: the order of col in a vector\n",
    "    : return: normalized matrix\n",
    "    '''\n",
    "    \n",
    "    scaler_path = Path(scaler_file).joinpath(str(eventId), str(col_ord) + 'scaler.save')\n",
    "    \n",
    "    if scaler_path.is_file():\n",
    "        scaler = joblib.load(scaler_path.as_posix())\n",
    "    \n",
    "    else:\n",
    "        scaler = StandardScaler() \n",
    "        # scaler = RobustScaler()\n",
    "        # scaler = Normalizer()\n",
    "        # reshape to 2D from 1D\n",
    "        col = np.array(col).reshape(-1,1)\n",
    "        scaler = scaler.fit(col)\n",
    "    \n",
    "    # standardilize column\n",
    "    sat_col = scaler.transform(col)\n",
    "    \n",
    "    return sat_col\n",
    "\n",
    "\n",
    "def split_data(data, n_steps):\n",
    "    '''\n",
    "\n",
    "    : param data: the matrix for one event cluster\n",
    "    '''\n",
    "    if isinstance(data, np.ndarray):\n",
    "        length = data.shape[0]\n",
    "    else:\n",
    "        length = len(data)\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(length):\n",
    "        # create the end of position\n",
    "        end_ix = i + n_steps\n",
    "        # check whether the index excesses the boundary\n",
    "        if end_ix > data.shape[0] -1:\n",
    "            break\n",
    "\n",
    "        # get the input and output for model\n",
    "        X_seq, Y_seq = data[i: end_ix], data[end_ix]\n",
    "        \n",
    "        # avoid arrays in a array\n",
    "        X.append(X_seq.tolist())\n",
    "        y.append(Y_seq.tolist())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def model_build_train(train_X, train_y, model_file):\n",
    "    '''\n",
    "        the step is default one\n",
    "    '''\n",
    "    earlystopping = EarlyStopping(monitor='loss', patience=10)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(8,activation='relu',input_shape = (train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "    model.add(LSTM(8,activation='relu'))\n",
    "    model.add(Dense(train_y.shape[1]))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "    # fit model with validation\n",
    "    model.fit(train_X, train_y, epochs=500, batch_size=16, callbacks = [earlystopping], validation_split=0.3, verbose=2, shuffle=False)\n",
    "    \n",
    "    # saving weights\n",
    "    model.save(model_file.as_posix())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def mean_square_error(y_true, y_pred):\n",
    "    ''' modified mse to compute squared error for parameter model evaluation\n",
    "\n",
    "    :param y_true: the test y --- array\n",
    "    :param y_pred: the predict y --- array\n",
    "    :return: the mean of errors, the errors list\n",
    "    '''\n",
    "    # define the minus between two values\n",
    "    # return original value\n",
    "    d_matrix = subtract(y_true, y_pred)\n",
    "    mses = []\n",
    "    print('The shape of minus matrix is: {}'.format(d_matrix.shape))\n",
    "    # compute mse for every row\n",
    "    for i in range(d_matrix.shape[0]):\n",
    "        # initialize to 0 for every new row\n",
    "        sum_minus = 0\n",
    "        for j in range(d_matrix.shape[1]):\n",
    "            sum_minus += d_matrix[i, j] * d_matrix[i, j]\n",
    "        # compute the mse for every row\n",
    "        mse = np.mean(sum_minus)\n",
    "        mses.append(mse)\n",
    "    return mses\n",
    "\n",
    "\n",
    "def model_predict(model, test_x, test_y):\n",
    "    ''' \n",
    "\n",
    "    '''\n",
    "    pre_y = model.predict(test_x, verbose=1)\n",
    "\n",
    "    return mean_square_error(test_y, pre_y)\n",
    "\n",
    "\n",
    "def confidence_interval(confidence, mse):\n",
    "    ''' function to compute the confidence interval boundaries\n",
    "\n",
    "    :param confidence: the confidence value or threshold, like 98%\n",
    "    :param mses_list: the errors list\n",
    "    :return: the boundaries\n",
    "    '''\n",
    "    # define the interval tuple\n",
    "    return st.t.interval(confidence, len(mse)-1, loc=np.mean(mse), scale=st.sem(mse))\n",
    "\n",
    "\n",
    "def anomaly_match(mses_list, fp_int, tp_int, eventId):\n",
    "    '''\n",
    "    : param mses_list: the list of mean square errors\n",
    "    : param file_number: the matrix order\n",
    "    : return: two thresholds (for false positive, true positive),\n",
    "             the indexes of anomaly logs and false positive logs\n",
    "    '''\n",
    "    # here we use the max value as the threshold\n",
    "    CI_fp1 = confidence_interval(fp_int, mses_list)\n",
    "    \n",
    "    # it is for the false positive detection\n",
    "    threshold1 = CI_fp1[1]\n",
    "    \n",
    "    CI_an = confidence_interval(tp_int, mses_list)\n",
    "    # save the result from prediction, index is the order in event matrix\n",
    "    seq_pre_dict = {'seq_para':[],'para_pred':[]}\n",
    "    # it is for the anomaly detection\n",
    "    threshold2 = CI_an[1]\n",
    "\n",
    "    print('[+] Reporting based on thresholds to match anomaly for Parameter Vector Model!')\n",
    "    \n",
    "    for i in range(len(mses_list)):\n",
    "        seq_pre_dict['seq_para'].append(i)\n",
    "        # default add 0 as normal\n",
    "        seq_pre_dict['para_pred'].append(0)\n",
    "        # compare the true positive predictions\n",
    "        if mses_list[i] > threshold2:\n",
    "            print('The {}th log in event {} sequence is potentially anomaly'.format(i, eventId))\n",
    "            seq_pre_dict['para_pred'][-1] = 1\n",
    "        # compare the false positive predictions\n",
    "        elif mses_list[i] > threshold1:\n",
    "            print('The {}th log in event {} sequence is false positive'.format(i, eventId))\n",
    "            seq_pre_dict['para_pred'][-1] = 2\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return threshold1, threshold2, seq_pre_dict\n",
    "\n",
    "\n",
    "def visual_mses(eventId, mses, threshold1, threshold2, CI1, CI2):\n",
    "    ''' visualize the mse\n",
    "\n",
    "    '''\n",
    "    # create the x axis labels\n",
    "    x_list = []\n",
    "    for i in range(len(mses)):\n",
    "        x_list.append(i)\n",
    "    if len(x_list) < 1:\n",
    "        return\n",
    "    else:\n",
    "        plt.plot(x_list, mses)\n",
    "        # add the threshold lines with percentage\n",
    "        plt.axhline(y=threshold1, color='b', linestyle=\"-\", label='CI={}'.format(CI1))\n",
    "        plt.axhline(y=threshold2, color='r', linestyle=\"-\", label='CI={}'.format(CI2))\n",
    "        plt.ylabel(\"Errors Values\")\n",
    "        # match the first num\n",
    "        plt.title('Event '+ str(eventId) + ' ' + 'Errors Distribution')\n",
    "        plt.legend()\n",
    "        plt.show(block=False)\n",
    "        plt.pause(3)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def trace_seq_path(trace_df, seq_pre_dict, eventId, lab_encoder_file):\n",
    "    ''' generate dataframe to view the prediction\n",
    "\n",
    "    : param trace_df: the dataframe with numeric log key, record_id inside        \n",
    "    '''\n",
    "    # check whether the para_pred column has existed or not\n",
    "    if 'para_pred' not in trace_df:\n",
    "        # default assign 0\n",
    "        trace_df['para_pred'] = 0\n",
    "\n",
    "    lab_encoder = joblib.load(lab_encoder_file.as_posix())\n",
    "    # {eventId: log message}\n",
    "\n",
    "    event_log_map = dict(zip(lab_encoder.transform(lab_encoder.classes_), lab_encoder.classes_))\n",
    "    # extract the original log message indexes\n",
    "    ori_log = event_log_map[eventId]\n",
    "    eventId_indexes = trace_df[trace_df['log key'] == ori_log].index\n",
    "    \n",
    "    assert len(eventId_indexes) != len(seq_pre_dict['para_pred']), \" Length Not Matched \"\n",
    "    \n",
    "    for i, index in enumerate(eventId_indexes):\n",
    "        # replace the order with index\n",
    "        seq_pre_dict['seq_para'][i] = int(index)\n",
    "    \n",
    "    for ord_index, df_index in enumerate(seq_pre_dict['seq_para']):\n",
    "        trace_df['para_pred'][df_index] = seq_pre_dict['para_pred'][ord_index]\n",
    "\n",
    "    return trace_df\n",
    "\n",
    "\n",
    "def train_batch(para_model, model_file, batch_x, batch_y, steps, desired_thres, attempts):\n",
    "    ''' update model with false positve and corrected wrong prediction\n",
    "        stop train when predicted mes is smaller than a threshold or attempts reach a given num\n",
    "\n",
    "    : param para_model: the original trained model\n",
    "    : param batch_x: the x used to update the model\n",
    "    : param batch_y: the normal prediction\n",
    "    : param desired_thres: the threshold of confidence interval to match normal prediction\n",
    "    : param attempts: the threshold to stop the training\n",
    "    \n",
    "    : return: updated model with adjusted weights\n",
    "    '''\n",
    "    # train with batch data first\n",
    "    para_model.train_on_batch(batch_x, batch_y)\n",
    "    # check the predict result\n",
    "    mse_error = model_predict(para_model, batch_x, batch_y)\n",
    "    # calculate the value matched the desired threshold for CI\n",
    "    CI_AN = confidence_interval(desired_thres, mse_error)\n",
    "    # compare every mse with the CI_AN\n",
    "    for i in range(len(mse_error)):\n",
    "        # set the exit condition\n",
    "        success_flag = False\n",
    "        no_of_attempts = 0\n",
    "        # retrain if the mse is not acceptable\n",
    "        while mse_error[i] > CI_AN[1] and (no_of_attempts < attempts):\n",
    "            # convert 2D to 3D (samples, time steps, features)\n",
    "            batch_x_one = np.reshape(batch_x[i], (1,batch_x[i].shape[0], batch_x[i].shape[1]))\n",
    "            # convert 1D to 2D\n",
    "            batch_y_one = np.reshape(batch_y[i], (1, len(batch_y[i])))\n",
    "            para_model.fit(batch_x_one, batch_y_one)\n",
    "            \n",
    "            no_of_attempts += 1\n",
    "            mse_one = model_predict(para_model, batch_x_one, batch_y_one)\n",
    "            print(\"Attempt Number %d, Calculated error for this iteration %f\" %(no_of_attempts, mse_one[0]))\n",
    "\n",
    "            if mse_one < CI_AN[1]:\n",
    "                success_flag = True\n",
    "                break\n",
    "\n",
    "        if (success_flag == False) and (no_of_attempts >= attempts):\n",
    "            print(\"[-] Failed to incorporate this feedback\")\n",
    "\n",
    "        if success_flag == True:\n",
    "            print(\"[+] Feedback incorporated \\n\")\n",
    "            print(\"Took %d iterations to learn!\" %(no_of_attempts))\n",
    "\n",
    "    # saving weights\n",
    "    para_model.save(model_file.as_posix())\n",
    "\n",
    "    return para_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1047c9fc",
   "metadata": {},
   "source": [
    "## Execution Path Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ca1732f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def miss_rep_col(column_list):\n",
    "    if column_list.dtype.name != 'category':\n",
    "        # replace nan string to None\n",
    "        return column_list.replace(np.nan, \"None\")\n",
    "    else:\n",
    "        # replace nan integer to 0\n",
    "        return column_list.replace(np.nan, 0) \n",
    "\n",
    "\n",
    "def lab_enc(cate_list, label_encoder_file):\n",
    "    # feature engineering for list\n",
    "    cate_list_com = miss_rep_col(cate_list)\n",
    "    \n",
    "    if Path(label_encoder_file).is_file():\n",
    "        label_encoder = joblib.load(label_encoder_file)\n",
    "    else:\n",
    "        laber_encoder = LabelEncoder()\n",
    "        # key_log_arr = cate_list_com.values\n",
    "        label_encoder = laber_encoder.fit(cate_list_com)\n",
    "        # save the encoder for labelling\n",
    "        joblib.dump(label_encoder, label_encoder_file.as_posix())\n",
    "    \n",
    "    label_encode_cate = label_encoder.transform(cate_list_com)\n",
    "\n",
    "    return label_encode_cate\n",
    "\n",
    "\n",
    "def encode_key(key_log_series, label_encoder_file, n_steps):\n",
    "    ''' encode the string log key and one hot encode the number then\n",
    "\n",
    "    '''\n",
    "    label_encode_cate = lab_enc(key_log_series, label_encoder_file)\n",
    "    # load label_encoder to return number of classes\n",
    "    label_encoder = joblib.load(label_encoder_file)\n",
    "    class_num = len(label_encoder.classes_)\n",
    "    \n",
    "    X, y = split_data(label_encode_cate, n_steps)\n",
    "\n",
    "    # one hot encoding the labelled numeric log key array\n",
    "    one_hot_y = utils.to_categorical(y) # np_utils\n",
    "\n",
    "    X = np.array(X).astype(float)\n",
    "    \n",
    "    return X , one_hot_y, class_num\n",
    "\n",
    "\n",
    "def model_build(steps, class_num):\n",
    "    ''' build the model with two hidden layers\n",
    "    \n",
    "    : return: compiled model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    # input dim is the length of steps/history\n",
    "    model.add(Dense(16, input_dim=steps, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    # output unit is the number of classes\n",
    "    model.add(Dense(class_num,activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def split_data(key_log_list, n_steps):\n",
    "    X, y = [], []\n",
    "    for i in range(len(key_log_list)):\n",
    "        # create the end of position\n",
    "        end_ix = i + n_steps\n",
    "        # check whether the index excesses the boundary\n",
    "        if end_ix > len(key_log_list) -1:\n",
    "            break\n",
    "\n",
    "        # get the input and output for model\n",
    "        X_seq, Y_seq = key_log_list[i: end_ix], key_log_list[end_ix]\n",
    "        # avoid arrays in a array\n",
    "        X.append(X_seq.tolist())\n",
    "        y.append(Y_seq.tolist())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fit_eval(model, model_file, random_seed, X, y):\n",
    "    \n",
    "    if Path(model_file).is_file():\n",
    "        model = load_model(model_file.as_posix())# fit the model\n",
    "    else:\n",
    "        print(X)\n",
    "        earlystopping = EarlyStopping(monitor='accuracy', patience=10)\n",
    "        \n",
    "\n",
    "        # classifier = KerasClassifier(model, epochs=200, batch_size =5,  verbose=0)\n",
    "        # evaluate the model\n",
    "        # kfold = KFold(n_splits=10, shuffle=True, random_state=random_seed)\n",
    "        print('=================')\n",
    "        print(y)\n",
    "        # results = cross_val_score(classifier, X, y, cv=kfold)\n",
    "        # print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "        # in order to save model --- use fit\n",
    "        model.fit(X, y, epochs=500, batch_size =5, validation_split =0.2, callbacks = [earlystopping], verbose=2)\n",
    "    \n",
    "        # saving weights\n",
    "        model.save(model_file.as_posix())\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_sort(classifier, test_x, top_num):\n",
    "    '''\n",
    "    : param top_num: the top possibility to set as normal\n",
    "    '''\n",
    "    normal_result = []\n",
    "    results = classifier.predict_proba(test_x, batch_size=16, verbose=1)\n",
    "    # sort the result with shape of test_x.shape\n",
    "    ## iterate the rows\n",
    "    for row in range(results.shape[0]):\n",
    "        # sort descending\n",
    "        sorted_poss = [i for i in sorted(enumerate(results[row]), key=lambda x:x[1], reverse=True)]\n",
    "        # extract the top num of possibilities tuple as the normal (class_num, possibility)\n",
    "        normal_result.append(sorted_poss[: top_num])\n",
    "    \n",
    "    return normal_result\n",
    "\n",
    "\n",
    "def rebuild_data(test_x, normal_result):\n",
    "    ''' build normal sequences for updating \n",
    "\n",
    "    : param normal_result: checked predicted normal y -- list type with poss tuples inside\n",
    "    '''\n",
    "    assert test_x.shape[0] != len(normal_result), \"Array Shape does not Match\"\n",
    "    # build normal sequence --- kind of N-gram\n",
    "    normal_sequence = []\n",
    "    \n",
    "    for i in range(test_x.shape[0]):\n",
    "        test_seq = test_x[i].tolist()\n",
    "        normal_sequence.extend([ test_seq + [float(pro_y[0])]  for pro_y in normal_result[i]])\n",
    "\n",
    "    return normal_sequence\n",
    "\n",
    "    pprint(normal_sequence)\n",
    "\n",
    "\n",
    "def anomaly_match(classifier, test_x, test_y, threshold):\n",
    "    ''' match the top normal predict, anomaly logs if not matchable\n",
    "    \n",
    "\n",
    "    '''\n",
    "    # pre_y is a list of predicted class number\n",
    "    pre_y = classifier.predict_classes(test_x, batch_size=16, verbose=1)\n",
    "    # save the possible prediction\n",
    "    normal_result = []\n",
    "    # save the seq and the prediction\n",
    "    seq_pre_dict = {'seq_path':[], 'path_pred':[]}\n",
    "    results = classifier.predict_proba(test_x, batch_size=16, verbose=1)\n",
    "    # sort the result with shape of test_x.shape\n",
    "    ## iterate the rows\n",
    "    for row in range(results.shape[0]):\n",
    "        # sort descending\n",
    "        sorted_poss = [i for i in sorted(enumerate(results[row]), key=lambda x:x[1], reverse=True)]\n",
    "        # pprint(sorted_poss)\n",
    "        # print(sorted_poss)\n",
    "        # extract the top num of possibilities tuple as the normal (class_num, possibility)\n",
    "        pprint([clus_prob for clus_prob in sorted_poss if clus_prob[1] >= threshold])\n",
    "        normal_result.append([clus_prob for clus_prob in sorted_poss if clus_prob[1] >= threshold])\n",
    "\n",
    "    for i in range(len(pre_y)):\n",
    "        # default add 0 as the prediction result to result column\n",
    "        seq_pre_dict['seq_path'].append(list(test_x[i]) + [np.argmax(test_y[i])])\n",
    "        # 0 is the normal\n",
    "        seq_pre_dict['path_pred'].append(0)\n",
    "        # print(np.argmax(test_y[i]))\n",
    "        pprint([pre_y[0] for pre_y in normal_result[i]])\n",
    "        if np.argmax(test_y[i]) not in [pre_y[0] for pre_y in normal_result[i]]:\n",
    "            # change the prediction to anomaly with label 1\n",
    "            seq_pre_dict['path_pred'][-1] = 1\n",
    "            print(\"That is a potential anomaly prediction: {} from sequence {}\".format(np.argmax(test_y[i]), test_x[i]))\n",
    "\n",
    "    return seq_pre_dict    \n",
    "\n",
    "\n",
    "def trace_seq_path(trace_df, seq_pre_dict):\n",
    "    ''' generate dataframe to view the prediction\n",
    "\n",
    "    : param trace_df: the dataframe with numeric log key, record_id inside        \n",
    "    '''\n",
    "    for key, value in seq_pre_dict.items():\n",
    "        assert len(trace_df) == len(value)\n",
    "        trace_df[key] = value\n",
    "\n",
    "    return trace_df\n",
    "\n",
    "\n",
    "def train_batch(exec_model, model_file, batch_x, batch_y, desired_proba, attempts):\n",
    "    ''' update model with false positve and corrected wrong prediction\n",
    "        stop train when predicted proba larger than a given value or attempts reach a given value\n",
    "\n",
    "    : param exec_model: the original trained model\n",
    "    : param batch_x: the x used to update the model\n",
    "    : param batch_y: the normal or corrected y to update the prediction\n",
    "    : param desired_proba: the threshold to stop the train_on_batch\n",
    "    : param attempts: the threshold to stop the training\n",
    "    \n",
    "    : return: updated model with adjusted weights\n",
    "    '''\n",
    "    # train with batch data first\n",
    "    exec_model.train_on_batch(batch_x, batch_y)\n",
    "    # check the predict result\n",
    "    pred_y = exec_model.predict_proba(batch_x, verbose=2)\n",
    "    \n",
    "    for i in range(len(batch_y)):\n",
    "        # extract the predict proba for batch_y\n",
    "        pre_proba = pred_y[i][int(np.argmax(batch_y[i]))]\n",
    "        # set the exit condition\n",
    "        success_flag = False\n",
    "        no_of_attempts = 0\n",
    "        # retrain on the single input and output\n",
    "        while pre_proba <= desired_proba and (no_of_attempts<attempts):\n",
    "            print(pre_proba)            \n",
    "            exec_model.fit(np.reshape(batch_x[i],(1,-1)), np.reshape(batch_y[i],(1,-1)))\n",
    "            \n",
    "            no_of_attempts += 1\n",
    "\n",
    "            pred_one_y = exec_model.predict_proba(np.reshape(batch_x[i],(1,-1)), verbose=2)\n",
    "            pre_proba = pred_one_y[0][int(np.argmax(batch_y[i]))]\n",
    "            \n",
    "            print(\"Attempt Number %d, Predicted Proba for this iteration %f\" %(no_of_attempts, pre_proba))\n",
    "\n",
    "            if pre_proba > desired_proba:\n",
    "                success_flag = True\n",
    "                break\n",
    "\n",
    "        if (success_flag == False) and (no_of_attempts >= attempts):\n",
    "            print(\"[-] Failed to incorporate this feedback\")\n",
    "\n",
    "        if success_flag == True:\n",
    "            print(\"[+] Feedback incorporated \\n\")\n",
    "            print(\"Took %d iterations to learn!\" %(no_of_attempts))\n",
    "\n",
    "    # saving weights\n",
    "    exec_model.save(model_file.as_posix())\n",
    "\n",
    "    return exec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f762e4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
